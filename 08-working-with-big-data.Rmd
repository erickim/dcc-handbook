# Working with Big Data {#bigdata}


_Contributors: Kunal Mishra and Jade Benjamin-Chung_

## Basics
A pitfall of working in R is that all objects are stored in memory - this makes it very difficult to work with datasets that are larger than 1-2 Gb for most standard computers. Here, we'll explore some alternatives to working with big data in R, many of which avoid loading data into memory at all.

The Berkeley Statistical Computing Facility also has many good [training resources](http://statistics.berkeley.edu/computing/training).

## Using downsampled data
In our studies with very large datasets, we save "downsampled" data that usually includes a 1% random sample stratified by any important variables, such as year or household id. This allows us to efficiently write and test our code without having to load in large, slow datasets that can cause RStudio to freeze. Be very careful to be sure which dataset you are working with and to label results output accordingly. 

## Unix
Though bash is very commonly used for management of your file system, it is also a very capable at doing basic data manipulation with big data. At the core, since the data is stored on disk, you avoid having to overload memory when using bash commands as it will work with the files directly. By default, these commands will print the results to standard output (probably your terminal screen) and you can then redirect the results to other files on disk. These commands can also be chained via pipes (represented as `|`, similar to `%>%` in tidyverse). All of these have a list of arguments that can be passed in via flags (check the `man` page for more details on each).

| Command  | Description |
|----------|-------------|
| `head`/`tail` | Displays the first few or last few rows of a file |
| `cat` | Concatenates files and prints them |
| `sort` | Sorts the file |
| `cut` | Cuts out portions of each line and prints it |
| `grep` | Finds lines of a file that matches inputted patterns |
| `sed` | Find and replace |
| `awk` | Similar to `grep` and `sed` but with some extra programmatic functionality |
| `uniq` | Unifies repeated lines (combine with `sort` to get unique rows) |
| `wget` / `curl` | Downloads data/files from websites |

## SQL and `dbplyr`
SQL databases are relational databases that are a collection of _tables_ that consists of _fields_ or _attributes_, each containing a single _type_. If you use `dplyr` a lot, you will find that it is heavily inspired with a SQL flavor in mind. Formally, data gets loaded onto a database system and it is stored on disk. This alone makes working with data fast, but the real efficiency gain is the concept of indexing. If you are curious, most SQL databases implement their index with B trees or B+ trees, which allow for log time complexity for search operations in average and worst case scenarios while providing constant time complexity in best case scenario.

The basic structure of a SQL query is as follows:
```
SELECT [DISTINCT] (attributes)
FROM (table)
[WHERE (conditions)]
[GROUP BY (attributes) [HAVING (conditions)]]
[ORDER BY (attributes) [DESC]]
```

The equivalent `dplyr` command would look as such:
```
table %>%
  select(attributes) %>%     # distinct(attributes) for select distinct
  group_by(attributes) %>%   # 
  filter(conditions) %>%     # 
  arrange(attributes)        # arrange(desc(attributes)) for descending
```

There is ample support for connection to databases in R, and, in particular, there is the [`dbplyr`](https://dbplyr.tidyverse.org) package, which allows you to interface with the data with `dplyr` code instead of SQL code.

## `data.table` and `dtplyr`
It is often possible to load large datasets into memory in R, however computations will probably be very slow. One way around this is to use `data.table`. You will find that operations on data are much faster than base R or `dplyr` even though data is loaded into memory - this is because of clever programming in C as well as internally creating a _key_ (the SQL equivalent of an index) by default when loading in the data. You can improve on this even more by setting extra keys for variables you know you will be doing filter or join operations on. 

More recently from the tidyverse, is the implementation of [`dtplyr`](https://dtplyr.tidyverse.org), which allows for `dplyr` syntax on `data.table` objects.

An overview of the `dplyr` vs `data.table` debate can be found in [this stackoverflow post](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly/27840349#27840349) and all 3 answers are worth a read.

## `ff`, `bigmemory`, `biglm`
Sometimes, it may be impossible to load data into memory. Because of the overhead required, you can expect around twice as much memory needed as the size of the file on disk to just load in a sufficiently large dataset. One way to work around this is to keep the data on disk and instead create clever data structures that allow for natural interfacing with the data in an R session while mapping operations to the data on disk. Two packages that implement these ideas are `ff` and `bigmemory`.

We can now interface with the data while avoiding loading it into memory but we run into issues when we try to fit models on it. For an $n \times p$ dataset, linear regression has a time complexity of $O(np^2 + p^3)$ and a space complexity of $O(np + p^2)$ (this just means it will take a while and take up a lot of space for large $n$ and even moreso for large $p$). Clever solutions used in machine learning (think iterative algorithms stochastic gradient descent) can help us here. The idea is totake a smaller portion of our data, fit regression then update the coefficient based on another run of linear regression on another small portion of the data. For GLM models, this can be done with the `biglm` package which has integration with `ff` and `bigmemory`.

## Parallel computing








## Optimal RStudio set up 

Using the following settings will help ensure a smooth experience when working with big data. In RStudio, go to the "Tools" menu, then select "Global Options". Under "General":

**Workspace**

- **Uncheck** Restore RData into workspace at startup 
- Save workspace to RData on exit -- choose **never**

**History**

- **Uncheck** Always save history


Unfortunately RStudio often gets slow and/or freezes after hours working with big datasets. Sometimes it is much more efficient to just use Terminal / gitbash to run code and make updates in git.

