\documentclass[]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={Proctor Foundation Data Science Handbook},
            pdfauthor={Contributors: Ben Arnold, Jade Benjamin-Chung, Kunal Mishra, Anna Nguyen, Nolan Pokpongkiat, Stephanie Djajadi (many from UC Berkeley in addition to Proctor)},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{natbib}
\bibliographystyle{apalike}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{Proctor Foundation Data Science Handbook}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Contributors: Ben Arnold, Jade Benjamin-Chung, Kunal Mishra, Anna Nguyen, Nolan Pokpongkiat, Stephanie Djajadi (many from UC Berkeley in addition to Proctor)}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{2019-11-12}

\usepackage{booktabs}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter*{Welcome!}\label{welcome}}
\addcontentsline{toc}{chapter}{Welcome!}

Welcome to the Francis I. Proctor Foundation at the University of California, San Francisco (\url{https://proctor.ucsf.edu})!

This handbook summarizes some best practices for data science, drawing from our experience at the Francis I. Proctor Foundation and from that of our close colleagues in the Division of Epidemiology and Biostatistics at the University of California, Berkeley (where Prof.~Ben Arnold worked for many years before joining Proctor).

We do not intend this handbook to be a comprehensive guide to data science. Instead, it focuses more on practical, ``how-to'' guidance for conducting data science within epidemiologic research studies. Where possible, we reference existing materials and guides.

Although many of the ideas of environment-independent, the examples draw from the \href{https://cran.r-project.org/}{R} programming language. For an excellent overview of data science in R, see the book \href{https://r4ds.had.co.nz/}{R for Data Science}.

Much of the material in this handbook evolved from a version of Dr.~Jade Benjamin-Chung's \href{https://github.com/jadebc-berkeley/lab-manual}{lab manual} at the University of California, Berkeley. In addition to the Proctor team, many contributors include current and former students from UC Berkeley.

The last two chapters of the handbook cover our communication strategy and code of conduct for team members who work with Prof.~Ben Arnold, who leads Proctor's Data Coordinating Center. They summarize key pieces of a functional data science team. Although the last two chapters might be of interest to a broader circle, \emph{they are mostly relevant for people working directly with Ben.} Just because they are at the end does not make them less important.

It is a living document that we strive to update regularly. If you would like to contribute, please write Ben (\href{mailto:ben.arnold@ucsf.edu}{\nolinkurl{ben.arnold@ucsf.edu}}) and/or submit a pull request.

The GitHub repository for this handbook is: \url{https://github.com/proctor-ucsf/dcc-handbook}

\hypertarget{intro}{%
\chapter{Introduction: Work Flow and Reproducible Analyses}\label{intro}}

\emph{Contributors: Ben Arnold}

This handbook collates a number of tips to help organize the workflow of epidemiologic data analyses. There are probably a dozen good ways to organize a workflow for reproducible research. This document includes recommendations that arise from our own team's experience through numerous field trials and observational data analyses. The recommendations will not work for everybody or for all applications. But, they work well for most of us most of the time, else we wouldn't put in the time to share them.

Start with two organizing concepts:

\begin{itemize}
\item
  \textbf{Workflow}. Defined here as the process required to draw scientific inference from data collected in the field or lab. I.e., the process by which we take data, and then process it, share it internally, analyze it, and communicate results to the scientific community.
\item
  \textbf{Reproducible research}. A fundamental characteristic of the scientific method is that study findings can be reproduced beyond the original investigators. Data analyses that contribute to scientific research should be described and organized in a way that they could be reproduced by an independent person or research group. A data analysis that is not reproducible violates a core principle of the scientific method.
\end{itemize}

\hypertarget{workflow}{%
\section{Workflow}\label{workflow}}

Broadly speaking, a typical scientific data science work flow involves four steps to transform raw data (e.g., from the field) into summaries that communicate results to the scientific community.

\begin{figure}
\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/workflow} \caption{Overview of the four main steps in a typical data science workflow}\label{fig:fig-workflow}
\end{figure}

When starting a new project, the work flow tends to evolve gradually and by iteration. Data cleaning, data processing, exploratory analyses, back to data cleaning, and so forth. If the work takes place in an unstructured environment with no system to organize files and work flow, it rapidly devolves into into a disorganized mess; analyses become difficult or impossible to replicate and they are anything but scientific. Projects with short deadlines (e.g., proposals, conference abstract submissions, article revisions) are particularly vulnerable to this type of organizational entropy. Putting together a directory and workflow plan from the start helps keep files organized and prevent disorder. Modifications are inevitable -- as long as the system is organized, modifications are usually no problem.

Depending on the project, each step involves a different amount of work. Step 1 is by far the most time consuming, and often the most error-prone. We devote an entire chapter to it below (\protect\hyperlink{datacleaning}{Data cleaning and processing})

\hypertarget{reproducibility}{%
\section{Reproducibility}\label{reproducibility}}

As a guiding directive, this process should be reproducible. If you are not familiar with the concept of reproducible research, start with this manifesto (\href{https://www.nature.com/articles/s41562-016-0021}{Munafo et al.~2017}). For a deeper dive, we highly recommend the recent book from Christensen, Freese, and Miguel (\href{https://www.ucpress.edu/book/9780520296954/transparent-and-reproducible-social-science-research}{2019}). Although it is framed around social science, the ideas apply generally.

\hypertarget{automation}{%
\section{Automation}\label{automation}}

We recommend that the workflow be as automated as possible using a programming language. Automating the workflow in a programming language, and essentially reducing it to text, is advantageous because it makes the process transparent, well documented, easily modified, and amenable to version control; these characteristics lend themselves to reproducible research.

At Proctor, we mostly use R. With the development of \href{https://rstudio.com/}{Rstudio}, \href{https://rmarkdown.rstudio.com/}{R Markdown} and the \href{https://www.tidyverse.org/}{tidyverse} ecosystem (among others), the R language has evolved as much in the past few years as in all previous decades since its inception. This has made the conduct of automated, reproducible research considerably easier than it was 10 years ago.

\textbf{If you have a step in your analysis workflow that involves point-and-click or copy/paste, then STOP, and ask yourself (and your team): } \textbf{\emph{How can I automate this?}}

\hypertarget{workflows}{%
\chapter{Workflows}\label{workflows}}

\emph{Contributors: Ben Arnold}

A data science work flow typically progresses through 4 steps that rarely evolve in a purely linear fashion, but in the end should flow in this direction:

\begin{figure}
\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/workflow} \caption{Overview of the four main steps in a typical data science workflow}\label{fig:fig-workflow2}
\end{figure}

\begin{longtable}[]{@{}clcc@{}}
\caption{\label{tab:workflow} Workflow basics}\tabularnewline
\toprule
\begin{minipage}[b]{0.07\columnwidth}\centering
Steps\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\raggedright
Example activities\strut
\end{minipage} & \begin{minipage}[b]{0.28\columnwidth}\centering
\(\Rightarrow\) Inputs\strut
\end{minipage} & \begin{minipage}[b]{0.29\columnwidth}\centering
\(\Rightarrow\) Outputs\strut
\end{minipage}\tabularnewline
\midrule
\endfirsthead
\toprule
\begin{minipage}[b]{0.07\columnwidth}\centering
Steps\strut
\end{minipage} & \begin{minipage}[b]{0.25\columnwidth}\raggedright
Example activities\strut
\end{minipage} & \begin{minipage}[b]{0.28\columnwidth}\centering
\(\Rightarrow\) Inputs\strut
\end{minipage} & \begin{minipage}[b]{0.29\columnwidth}\centering
\(\Rightarrow\) Outputs\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.07\columnwidth}\centering
1\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
\textbf{Data cleaning and processing}\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\centering
.\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
make a plan for final datasets, fix data entry errors, create derived variables, plan for public replication files\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
untouched datasets\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
final datasets\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\centering
2-3\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
\textbf{Analyses}\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\centering
.\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
exploratory data analysis, study monitoring, summary statistics, statistical analyses, independent replication of analyses, make figures and tables\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
final datasets\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
saved results (.rds/.csv), tables (.html,.pdf), figures (.html/.png)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\centering
4\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
\textbf{Communication}\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.07\columnwidth}\centering
.\strut
\end{minipage} & \begin{minipage}[t]{0.25\columnwidth}\raggedright
results synthesis\strut
\end{minipage} & \begin{minipage}[t]{0.28\columnwidth}\centering
saved results, figures, tables\strut
\end{minipage} & \begin{minipage}[t]{0.29\columnwidth}\centering
monitoring reports, presentations, scientific articles\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

In many modern data science workflows, steps 2-4 can be accomplished in a single R notebook or Jupyter notebook: the statistical analysis, creation of figures and tables, and creation of reports.

However, it is still useful to think of the distinct stages in many cases. For example, a single statistical analysis might contribute to a DSMC report, a scientific conference presentation, and a scientific article. In this example, each piece of scientific communication would take the same input (stored analysis results as .csv/.rds) and then proceed along slightly different downstream workflows.

It would be more error prone to replicate the same statistical analysis in three parallel downstream work flows. This illustrates a key idea that holds more generally:

\begin{longtable}[]{@{}l@{}}
\toprule
\endhead
\textbf{Key idea for workflows:} Whenever possible, avoid repeating the same data processing or statistical analysis in separate streams. \emph{Key data processing and analyses should be done once}.\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{directory-structure-and-code-repositories}{%
\chapter{Directory Structure and Code Repositories}\label{directory-structure-and-code-repositories}}

\emph{Contributors: Kunal Mishra, Jade Benjamin-Chung, Ben Arnold}

The backbone of your project workflow is the file directory so it makes sense to spend time organizing the directory. Note that \texttt{directory} is the technical term for the system used to organize individual files. Most non-UNIX environments use a folder analogy, and directory and folder can be used interchangeably in a lot of cases. A well organized directory will make everything that follows much easier. Just like a well designed kitchen is essential to enjoy cooking (and avoid clutter), a well designed directory helps you enjoy working and stay organized in a complex project with literally thousands of related files. Just like a disorganized kitchen (``now where did I put that spatula?'') a disorganized project directory creates confusion, lost time, stress, and mistakes.

Another huge advantage of maintaining a regular/predictable directory structure within a project and across projects is that it makes it more intuitive. When a directory is intuitive, it is easier to work collaboratively across a larger team; everybody can predict (at least approximately) where files should be.

Nested within your directory will be a \texttt{code\ repository}. Sometimes we find it useful to manage the code repository using version control, such as git/GitHub.

Other chapters will discuss \protect\hyperlink{codingpractices}{coding practices}, \protect\hyperlink{datamanagement}{data management}, and \protect\hyperlink{github}{GitHub/version control} that will build from the material here.

Carrying the kitchen analogy further: here, we are designing the kitchen. Then, we'll discuss approaches for how to cook in the kitchen that we designed/built.

\hypertarget{small-and-large-projects}{%
\section{Small and large projects}\label{small-and-large-projects}}

Our experience is that the overwhelming majority of projects come in two sizes: small and large. We recommend setting up your directory structure depending on how large you expect the project to be. Sometimes, small projects evolve into large projects, but only occasionally. A small project is something like a single data analysis with a single published article in mind. A large project is an epidemiologic field study, where there are multiple different types of data and different types of analyses (e.g., sample size calculations, survey data, biospecimens, substudies, etc.).

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\textbf{Small project:} There is essentially one dataset and a single, coherent analysis. For example, a simulation study or a methodology study that will lead to a single article.

\textbf{Large project:} A field study that includes multiple activities, each of which generates data files. Multiple analyses are envisioned, leading to multiple scientific articles.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

Large projects are more common and more complicated. Most of this chapter focuses on large project organization (small projects can be thought of as essentially one piece of a large project).

\hypertarget{directory-structure}{%
\section{Directory Structure}\label{directory-structure}}

In the example below, we follow a basic directory naming convention that makes working in UNIX and typing directory statements in programs much easier:

\begin{itemize}
\tightlist
\item
  \textbf{short names}
\item
  \textbf{no spaces in the names} (not essential but a personal preference. Can use \texttt{\_} or \texttt{-} instead)
\item
  \textbf{lower case} (not essential, again, personal preferences vary!)
\end{itemize}

For example, Ben completed a study in Tamil Nadu, India during his dissertation to study the effect of improvements in water supply and sanitation on child health. Instead of naming the directory \texttt{Tamil\ Nadu} or \texttt{Tamil\ Nadu\ WASH\ Study}, he used \texttt{trichy} instead (a colloquial name for the city near the study, Tiruchirappalli), which was much easier to type in the terminal and in directory statements. A short name helps make directory references easier while programming.

\hypertarget{first-level-data-and-analyses}{%
\subsection{First level: data and analyses}\label{first-level-data-and-analyses}}

\begin{figure}
\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/mystudy-dir-fig1} \caption{Example directory for `mystudy`}\label{fig:dir-fig1}
\end{figure}

Start by dividing a project into major activities. In the example abpve, the project is named \texttt{mystudy}. There is a \texttt{data} subdirectory (more in a sec), and then three major activities, each corresponding to a separate analysis: \texttt{primary-analysis},\texttt{secondary-analysis-1}, and \texttt{secondary-analysis-2}. In a real project, the names could be more informative, such as ``trachoma-qpcr''. Also, a real project might also include many additional subdirectories related to administrative and logistics activities that do not relate to data science, such as irb, travel, contracts, budget, survey forms, etc.).

Dividing files into major activities helps keep things organized for really big projects. In a multi-site study, consider including a directory for each site before splitting files into major activities. Ideally, analyses will not span major activity subdirectories in a project folder, but sometimes you can't predict/avoid that from happening.

\hypertarget{second-level-data}{%
\subsection{Second level: data}\label{second-level-data}}

\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/mystudy-dir-fig2}

Each project will include a \texttt{data} directory. We recommend organizing it into 3 parts: \texttt{untouched}, \texttt{temp}, and \texttt{final}. Often, it is useful to include a fourth subdirectory called \texttt{public} for sharing public versions of datasets.

The \texttt{untouched} directory includes all untouched datasets that are used for the study. Once saved in the directory never touch them; you will read them into the work flow, but \textbf{never, ever save over them}. If the study has repeated extracts from rolling data collection or electronic health records, consider subdirectories within \texttt{untouched} that are indexed by date.

The \texttt{temp} directory (optional, not essential) includes temporary files that you might generate during the data management process. This is mainly a space for experimentation. As a rule, never save anything in the temp directory that you cannot delete. Regularly delete files in the temp directory to save disk space.

The \texttt{final} directory includes final datasets for the activity. Final datasets are de-identified and require no further processing; they are clean and ready for analysis. They should be accompanied by meta-data, which at minimum includes the data's provenance (i.e., how it was created) and what it includes (i.e., level of the data, plus variable coding/labels/descriptions). Clean/final datasets generated by one analysis might be reused in another.

\hypertarget{second-level-analysis}{%
\subsection{Second level: analysis}\label{second-level-analysis}}

We recommend maintaining a separate subdirectory for each major analysis in a project. In this example, there are three with not-very-creative names from the view of trial: \texttt{primary-analysis}, \texttt{secondary-analysis-1}, \texttt{secondary-analysis-2}.

Think of each analysis as the scope of all of the work for a single, published paper. We recommend dividing the analysis project into a space for computational notebooks / scripts (i.e., a \texttt{code\ repository}), and a second for their output. The reason for the split is to make it easier to use version control (should you choose) for the code. Version control like \texttt{git} and \texttt{GitHub} (see the \protect\hyperlink{GitHub}{Chapter on GitHub}) works well for text files but isn't really designed for binary files such as images (.png), datasets (.rds), or PDF files (.pdf). It is certainly possible to use git with those file types, but since git makes a new copy of the file every time it is changed the git repo can get horribly bloated and takes up too much space on disk. Consolidating the output into a separate directory makes it more obvious that it isn't under version control. In this example, there are separate parts for code (\texttt{R}) and output (\texttt{output}). Output could include figures, tables, or saved analysis results stored as data files (.rds or .csv). Another conventional name for the code repository is \texttt{src} as an alternative to \texttt{R} if you use other languages.

\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/mystudy-dir-fig3}

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\emph{Interdependence between analyses}: Sometimes a result from an analysis might be a cleaned dataset that could feed into future, distinct analyses. This is quite common, for example, in large trials where a set of baseline characteristics might be used in multiple separate papers for different endpoints, either for assessing balance of the trial population or subgroups, or used as adjustment covariates in additional analyses of the trial. In this case, the cleaned dataset would be written to the \texttt{data/final} directory and is thus available for future use.

\begin{center}\rule{0.5\linewidth}{\linethickness}\end{center}

\hypertarget{code-repositories}{%
\section{Code Repositories}\label{code-repositories}}

Maintain a separate code repository for each major analysis activity (last section).

We recommend the following structure for a code repository:

\includegraphics[width=0.75\linewidth]{/Users/benarnold/dcc-handbook/images/mystudy-dir-fig4}

With subdirectories that generally look like this:

\begin{verbatim}
.gitignore
primary-analysis.Rproj
0-config.R
0-shared-functions.R
0-primary-analysis-run-all.sh
1-dm /
    0-dm-run-all.sh
    1-format-enrollment-data.R
    2-format-adherence-data.R
    3-format-LAZ-measurements.R
2-baseline-balance /
    0-baseline-balance-run-all.sh
    ...
3-consort /
    0-consort-run-all.sh
    ...
4-adherence /
    0-adherence-run-all.sh
    ...
5-laz /
    0-laz-run-all.sh
    1-laz-unadjusted-analysis.R
    2-laz-adjusted-analysis.R
6-figures / 
    0-figures-run-all.sh
    Fig1-consort.Rmd
    Fig2-adherence.Rmd
    Fig3-laz.Rmd
\end{verbatim}

Note \texttt{dm} is shorthand for ``data management.'' You can call the data management directory anything you want, but just ensure that you have one. This helps ensure work conducted in step 1 of your workflow stays upstream from all analyses (see \protect\hyperlink{workflows}{Chapter on workflows}). Also note that in this example, all of the scripts are \texttt{.R} files. Increasingly, we use R Markdown notebooks \texttt{.Rmd} instead of / in addition to R files.

For brevity, we haven't expanded every directory, but you can glean some important takeaways from what you \emph{do} see.

\hypertarget{rproj-files}{%
\subsection{\texorpdfstring{\texttt{.Rproj} files}{.Rproj files}}\label{rproj-files}}

An ``R Project'' can be created within RStudio by going to \texttt{File\ \textgreater{}\textgreater{}\ New\ Project}. Depending on where you are with your research, choose the most appropriate option. This will save preferences, working directories, and even the results of running code/data (though we recommend starting from scratch each time you open your project, in general). Then, ensure that whenever you are working on that specific research project, you open your created project to enable the full utility of \texttt{.Rproj} files. This also automatically sets the directory to the top level of the project.

\hypertarget{configuration-config-file}{%
\subsection{Configuration (`config') File}\label{configuration-config-file}}

This is the single most important file for your project. It will be responsible for a variety of common tasks, declare global variables, load functions, declare paths, and more. \emph{Every other file in the project} will begin with \texttt{source("0-config")}, and its role is to reduce redundancy and create an abstraction layer that allows you to make changes in one place (\texttt{0-config.R}) rather than 5 different files. To this end, paths that will be referenced in multiple scripts (e.g., a \texttt{clean\_data\_path}) can be declared in \texttt{0-config.R} and simply referred to by its variable name in scripts. If you ever want to change things, rename them, or even switch from a downsample to the full data, all you would then to need to do is modify the path in one place and the change will automatically update throughout your project. See the example config file for more details. The paths defined in the \texttt{0-config.R} file assume that users have opened the \texttt{.Rproj} file, which sets the directory to the top level of the project.

This \href{https://github.com/jadebc-berkeley/WBB-STH-Kato-Katz}{GitHub repository} that has replication files for \href{https://www.biorxiv.org/content/10.1101/629501v1}{this study} includes an example of a streamlined \texttt{config.R} file, with all packages loaded and directory references defined.

\hypertarget{shared-functions-file}{%
\subsection{Shared Functions File}\label{shared-functions-file}}

If you write a custom function for an analysis and need to use it repeatedly across multiple analysis scripts, then it is better to consolidate it into a single shared functions script and source that file into the analysis scripts. The reason for this is that it enables you to edit the function in a single place and ensure that the changes are implemented across your entire workflow. In extreme cases, you might have so many shared functions that you need an entire subdirectory with separate scripts. \href{https://github.com/HBGD-UCB/ki-longitudinal-manuscripts/}{This repository} includes an example (\texttt{0-project-functions}) of a large analysis (currently still a work in progress).

\hypertarget{order-files-and-subdirectories}{%
\subsection{Order Files and Subdirectories}\label{order-files-and-subdirectories}}

This makes the jumble of alphabetized filenames much more coherent and places similar code and files next to one another. Although sometimes there is not a linear progression from 1 to 2 to 3, in general the structure helps reflect how data flows from start to finish and allows us to easily map a script to its output (i.e.~\texttt{primary-analysis/R/5-laz/1-laz-unadjusted-analysis.R} =\textgreater{} \texttt{primary-analysis/output/5-laz/1-laz-unadjusted-analysis.RDS}). That is, the code repository and the output are approximately mirrored. If you take nothing else away from this guide, this is the single most helpful suggestion to make your workflow more coherent. Often the particular order of files will be in flux until an analysis is close to completion. At that time it is important to review file order and naming and reproduce everything prior to drafting a manuscript.

In the \texttt{6-figures} subdirectory, each RMarkdown file (computational notebook) is linked to a specific figure in a hypothetical manuscript. This makes it easier to link specific notebooks in figure legends, and to see which file creates each figure.

\hypertarget{use-bash-scripts-to-ensure-reproducibility}{%
\subsection{Use Bash scripts to ensure reproducibility}\label{use-bash-scripts-to-ensure-reproducibility}}

Bash scripts are useful components of a reproducible workflow. At many of the directory levels (i.e.~in \texttt{5-laz}), there is a bash script that runs each of the analysis scripts. This is exceptionally useful when data ``upstream'' changes -- you simply run the bash script. See the \protect\hyperlink{unix}{UNIX Chapter} for further details.

\hypertarget{alternative-approach-for-code-repos}{%
\subsection{Alternative approach for code repos}\label{alternative-approach-for-code-repos}}

Another approach for organizing your code repository is to name all of your scripts according to the final figure or table that they generate for a particular article. In our experience, this \emph{only} works for small projects, with a single set of coherent analyses. Here, you might have an alternative structure such as:

\begin{verbatim}
.gitignore
primary-analysis.Rproj
0-config.R
0-shared-functions.R
0-primary-analysis-run-all.sh
1-dm /
    0-dm-run-all.sh
    1-format-enrollment-data.R
    2-format-adherence-data.R
    3-format-LAZ-measurements.R
Fig1-consort.Rmd
Fig2-adherence.Rmd
Fig3-1-laz-analysis.Rmd
Fig3-2-laz-make-figure.Rmd
\end{verbatim}

There is still a need for a separate data management directory (e.g., \texttt{dm}) to ensure that workflow is upstream from the analysis (more below in \protect\hyperlink{unix}{chapter on UNIX}), but then scripts are all together with clear labels. If a figure requires two stages to the analysis, then you can name them sequentially, such as \texttt{Fig3-1-laz-analysis.Rmd}, \texttt{Fig3-2-laz-make-figure.Rmd}. There is no way to divine how all of the analyses will neatly fit into files that correspond to separate figures. Instead, they will converge on these file names through the writing process, often through comsolidation or recombination.

One example of a small repo is here:
\url{https://github.com/ben-arnold/enterics-seroepi}

\hypertarget{codingpractices}{%
\chapter{Coding Practices}\label{codingpractices}}

\emph{Contributors: Kunal Mishra, Jade Benjamin-Chung, Ben Arnold}

\hypertarget{organizing-scripts}{%
\section{Organizing scripts}\label{organizing-scripts}}

Just as your data ``flows'' through your project, data should flow naturally through a script. Very generally, you want to:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  describe the work completed in the script in a comment header
\item
  source your configuration file (\texttt{0-config.R})
\item
  load all of your data
\item
  do all your analysis/computation in order
\item
  save your results
\end{enumerate}

Each section should be ``chunked together'' using comments, often with many chunks in a single section. See \href{https://github.com/kmishra9/Flu-Absenteeism/blob/master/Master's\%20Thesis\%20-\%20Spatial\%20Epidemiology\%20of\%20Influenza/2a\%20-\%20Statistical-Inputs.R}{this file} for a good example of how to cleanly organize a file in a way that follows this ``flow'' and functionally separate pieces of code that are doing different things. \href{https://github.com/ben-arnold/enterics-seroepi/blob/master/R/Fig1-haiti-ab-distributions.Rmd}{This is another example} in a \texttt{.Rmd} format, where chunking is made even more obvious by the interleaving of markdown text and R code in the same notebook file.

\hypertarget{documenting-your-code}{%
\section{Documenting your code}\label{documenting-your-code}}

\hypertarget{file-headers}{%
\subsection{File headers}\label{file-headers}}

Every file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. You may optionally wish to include the inputs and outputs of the script as well, though the next section makes this significantly less necessary. It can be very helpful to include your name and email address as well so others can identify who wrote the code. This is unnecessary if you are using version control (\texttt{git/GitHub}) becuase that information will be tracked by commits.

\begin{verbatim}
#-------------------------------------------------------------------------------
# @Organization - Example Organization
# @Project - Example Project
# @Author - Your name, and possibly email address (if appropriate)
# @Description - This file is responsible for [...]
#-------------------------------------------------------------------------------
\end{verbatim}

Consider using RStudio's \href{https://support.rstudio.com/hc/en-us/articles/200484568-Code-Folding-and-Sections}{code folding} feature to collapse and expand different sections of your code. Any comment line with at least four trailing dashes (-), equal signs (=), or pound signs (\#) automatically creates a code section. Delimiters for chunks are a personal preference and all work equally well. For example:

\begin{verbatim}
# Section 1 ------------------
\end{verbatim}

works equally well as

\begin{verbatim}
##############################
# Section 1 ##################
##############################
\end{verbatim}

\hypertarget{comments-in-the-body-of-your-script}{%
\subsection{Comments in the body of your script}\label{comments-in-the-body-of-your-script}}

Commenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you'll be thankful for comments. When you revisit code you wrote two years earlier, you'll be thankful for comments. There's no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See \href{https://github.com/kmishra9/Flu-Absenteeism/blob/master/Master's\%20Thesis\%20-\%20Spatial\%20Epidemiology\%20of\%20Influenza/1b\%20-\%20Map-Management.R}{this file} for an example of how to comment your code and notice that comments are always in the form of:

\texttt{\#\ This\ is\ a\ comment\ -\/-\ first\ letter\ is\ capitalized\ and\ spaced\ away\ from\ the\ pound\ sign}

\hypertarget{function-documentation}{%
\subsection{Function documentation}\label{function-documentation}}

Every function you write must include a header to document its purpose, inputs, and outputs. For any reproducible workflows, they are essential, because R is dynamically typed. This means, you can pass a \texttt{string} into an argument that is meant to be a \texttt{data.table}, or a \texttt{list} into an argument meant for a \texttt{tibble}. It is the responsibility of a function's author to document what each argument is meant to do and its basic type. This is an example for documenting a function (inspired by \href{https://www.oracle.com/technetwork/java/javase/documentation/index-137868.html\#format}{JavaDocs}, R's \href{https://blog.rstudio.com/2018/10/23/rstudio-1-2-preview-plumber-integration/}{Plumber API docs}, and \href{https://kbroman.org/pkg_primer/pages/docs.html}{Roxygen2}):

\begin{verbatim}
#-----------------------------------------------------------------------------
# Documentation: calc_fluseas_mean
# Usage: calc_fluseas_mean(data, yname)
# @description: Make a dataframe with rows for flu season and site
#               and the number of patients with an outcome, the total patients,
#               and the percent of patients with the outcome

# Arguments/Options:
# @param data: a data frame with variables flu_season, site, studyID, and yname
# @param yname: a string for the outcome name
# @param silent: a boolean specifying whether the function shouldn't output anything to the console (DEFAULT: TRUE)

# @return: the dataframe as described above
# @output: prints the data frame described above if silent is not True
#-----------------------------------------------------------------------------

calc_fluseas_mean = function(data, yname, silent = TRUE) {
 ### function code here 

}
\end{verbatim}

The header tells you what the function does, its various inputs, and how you might go about using the function to do what you want. Also notice that all optional arguments (i.e.~ones with pre-specified defaults) follow arguments that require user input.

\begin{itemize}
\item
  \textbf{Note}: As someone trying to call a function, it is possible to access a function's documentation (and internal code) by \texttt{CMD-Left-Click}ing the function's name in RStudio
\item
  \textbf{Note}: Depending on how important your function is, the complexity of your function code, and the complexity of different types of data in your project, you can also add ``type-checking'' to your function with the \texttt{assertthat::assert\_that()} function. You can, for example, \texttt{assert\_that(is.data.frame(statistical\_input))}, which will ensure that collaborators or reviewers of your project attempting to use your function are using it in the way that it is intended by calling it with (at the minimum) the correct type of arguments. You can extend this to ensure that certain assumptions regarding the inputs are fulfilled as well (i.e.~that \texttt{time\_column}, \texttt{location\_column}, \texttt{value\_column}, and \texttt{population\_column} all exist within the \texttt{statistical\_input} tibble).
\end{itemize}

\hypertarget{object-naming}{%
\section{Object naming}\label{object-naming}}

Generally we recommend using nouns for objects and verbs for functions. This is because functions are performing actions, while objects are not.

Try to make your variable names both more expressive and more explicit. Being a bit more verbose is useful and easy in the age of autocompletion! For example, instead of naming a variable \texttt{vaxcov\_1718}, try naming it \texttt{vaccination\_coverage\_2017\_18}. Similarly, \texttt{flu\_res} could be named \texttt{absentee\_flu\_residuals}, making your code more readable and explicit.

\begin{itemize}
\tightlist
\item
  For more help, check out \href{https://spin.atomicobject.com/2017/11/01/good-variable-names/}{Be Expressive: How to Give Your Variables Better Names}
\end{itemize}

We recommend you use \textbf{Snake\_Case}.

\begin{itemize}
\item
  Base R allows \texttt{.} in variable names and functions (such as \texttt{read.csv()}), but this goes against best practices for variable naming in many other coding languages. For consistency's sake, \texttt{snake\_case} has been adopted across languages, and modern packages and functions typically use it (i.e.~\texttt{readr::read\_csv()}). As a very general rule of thumb, if a package you're using doesn't use \texttt{snake\_case}, there may be an updated version or more modern package that \emph{does}, bringing with it the variety of performance improvements and bug fixes inherent in more mature and modern software.
\item
  \textbf{Note}: you may also see \texttt{camelCase} throughout the R code you come across. This is \emph{okay} but not ideal -- try to stay consistent across all your code with \texttt{snake\_case}.
\item
  \textbf{Note}: there is nothing inherently wrong with using \texttt{.} in variable names, just that it goes against style best practices that are cropping up in data science, so its worth getting rid of these bad habits now.
\end{itemize}

\hypertarget{function-calls}{%
\section{Function calls}\label{function-calls}}

In a function call, use ``named arguments'' and separate arguments by to make your code more readable.

Here's an example of what not to do when calling the function a function \texttt{calc\_fluseas\_mean} (defined above):

\begin{verbatim}
mean_Y = calc_fluseas_mean(flu_data, "maari_yn", FALSE)
\end{verbatim}

And here it is again using the best practices we've outlined:

\begin{verbatim}
mean_Y = calc_fluseas_mean(
  data = flu_data, 
  yname = "maari_yn",
  silent = FALSE
)
\end{verbatim}

\hypertarget{the-here-package}{%
\section{\texorpdfstring{The \texttt{here} package}{The here package}}\label{the-here-package}}

The \texttt{here} package is one great R package that helps multiple collaborators deal with the mess that is working directories within an R project structure. Let's say we have an R project at the path \texttt{/home/oski/Some-R-Project}. My collaborator might clone the repository and work with it at some other path, such as \texttt{/home/bear/R-Code/Some-R-Project}. Dealing with working directories and paths explicitly can be a very large pain, and as you might imagine, setting up a Config with paths requires those paths to flexibly work for all contributors to a project. This is where the \texttt{here} package comes in and this a \href{https://github.com/jennybc/here_here}{great vignette describing it}.

For more motivation on why you should use the \texttt{here} and R projects (\texttt{.Rproj}), read this excellent blog post from \href{https://www.tidyverse.org/articles/2017/12/workflow-vs-script/}{Tidyverse}.

\hypertarget{tidyverse}{%
\section{Tidyverse}\label{tidyverse}}

Throughout this document there have been references to the Tidyverse, but this section is to explicitly show you how to transform your Base R tendencies to Tidyverse (or Data.Table, Tidyverse's performance-optimized competitor). For most of our work that does not utilize very large datasets, we recommend that you code in Tidyverse rather than Base R. Tidyverse is quickly becoming \href{https://rviews.rstudio.com/2017/06/08/what-is-the-tidyverse/}{the gold standard} in R data analysis and modern data science packages and code should use Tidyverse style and packages unless there's a significant reason not to (i.e.~big data pipelines that would benefit from Data.Table's performance optimizations).

The package author has published a \href{https://r4ds.had.co.nz/}{great textbook on R for Data Science}, which leans heavily on many Tidyverse packages and may be worth checking out.

The following list is not exhaustive, but is a compact overview to begin to translate Base R into something better:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.47\columnwidth}\raggedright
Base R\strut
\end{minipage} & \begin{minipage}[b]{0.47\columnwidth}\raggedright
Better Style, Performance, and Utility\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{read.csv()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{readr::read\_csv()} or \texttt{data.table::fread()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{write.csv()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{readr::write\_csv()} or \texttt{data.table::fwrite()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{readRDS}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{readr::read\_rds()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{saveRDS()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{readr::write\_rds()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{data.frame()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{tibble::tibble()} or \texttt{data.table::data.table()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{rbind()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{dplyr::bind\_rows()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{cbind()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{dplyr::bind\_cols()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\$some\_column}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\ \%\textgreater{}\%\ dplyr::pull(some\_column)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\$some\_column\ =\ ...}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\ \%\textgreater{}\%\ dplyr::mutate(some\_column\ =\ ...)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df{[}get\_rows\_condition,{]}}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\ \%\textgreater{}\%\ dplyr::filter(get\_rows\_condition)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df{[},c(col1,\ col2){]}}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df\ \%\textgreater{}\%\ dplyr::select(col1,\ col2)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{merge(df1,\ df2,\ by\ =\ ...,\ all.x\ =\ ...,\ all.y\ =\ ...)}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{df1\ \%\textgreater{}\%\ dplyr::left\_join(df2,\ by\ =\ ...)} or \texttt{dplyr::full\_join} or \texttt{dplyr::inner\_join} or \texttt{dplyr::right\_join}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\_\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{str()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{dplyr::glimpse()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{grep(pattern,\ x)}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{stringr::str\_which(string,\ pattern)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{gsub(pattern,\ replacement,\ x)}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{stringr::str\_replace(string,\ pattern,\ replacement)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{ifelse(test\_expression,\ yes,\ no)}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{if\_else(condition,\ true,\ false)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
Nested: \texttt{ifelse(test\_expression1,\ yes1,\ ifelse(test\_expression2,\ yes2,\ ifelse(test\_expression3,\ yes3,\ no)))}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{case\_when(test\_expression1\ \textasciitilde{}\ yes1,\ \ test\_expression2\ \textasciitilde{}\ yes2,\ test\_expression3\ \textasciitilde{}\ yes3,\ TRUE\ \textasciitilde{}\ no)}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{proc.time()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{tictoc::tic()} and \texttt{tictoc::toc()}\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{stopifnot()}\strut
\end{minipage} & \begin{minipage}[t]{0.47\columnwidth}\raggedright
\texttt{assertthat::assert\_that()} or \texttt{assertthat::see\_if()} or \texttt{assertthat::validate\_that()}\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

For a more extensive set of syntactical translations to Tidyverse, you can check out \href{https://tavareshugo.github.io/data_carpentry_extras/base-r_tidyverse_equivalents/base-r_tidyverse_equivalents.html\#reshaping_data}{this document}.

Working with Tidyverse within functions can be somewhat of a pain due to non-standard evaluation (NSE) semantics. If you're an avid function writer, we'd recommend checking out the following resources:

\begin{itemize}
\tightlist
\item
  \href{https://www.youtube.com/watch?v=nERXS3ssntw}{Tidy Eval in 5 Minutes} (video)
\item
  \href{https://tidyeval.tidyverse.org/index.html}{Tidy Evaluation} (e-book)
\item
  \href{https://www.brodrigues.co/blog/2016-07-18-data-frame-columns-as-arguments-to-dplyr-functions/}{Data Frame Columns as Arguments to Dplyr Functions} (blog)
\item
  \href{https://stackoverflow.com/questions/28125816/r-standard-evaluation-for-join-dplyr}{Standard Evaluation for *\_join} (stackoverflow)
\item
  \href{https://dplyr.tidyverse.org/articles/programming.html}{Programming with dplyr} (package vignette)
\end{itemize}

\hypertarget{coding-with-r-and-python}{%
\section{Coding with R and Python}\label{coding-with-r-and-python}}

If you're using both R and Python, you may wish to check out the \href{https://www.rdocumentation.org/packages/feather/versions/0.3.3}{Feather package} for exchanging data between the two languages \href{https://blog.rstudio.com/2016/03/29/feather/}{extremely quickly}.

\hypertarget{codingstyle}{%
\chapter{Coding Style}\label{codingstyle}}

\emph{Contributors: Kunal Mishra, Jade Benjamin-Chung, and Ben Arnold}

\hypertarget{line-breaks}{%
\section{Line breaks}\label{line-breaks}}

\begin{itemize}
\item
  For \texttt{ggplot} calls and \texttt{dplyr} pipelines, do not crowd single lines. Here are some nontrivial examples of ``beautiful'' pipelines, where beauty is defined by coherence:

\begin{verbatim}
# Example 1
school_names <- list(
  OUSD_school_names = absentee_all %>%
    filter(dist.n == 1) %>%
    pull(school) %>%
    unique() %>%
    sort(),

  WCCSD_school_names <- absentee_all %>%
    filter(dist.n == 0) %>%
    pull(school) %>%
    unique() %>%
    sort()
)
\end{verbatim}

\begin{verbatim}
# Example 2
absentee_all <- fread(file = raw_data_path) %>%
  mutate(program = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,
                             schoolyr %in% program_schoolyrs ~ 1)) %>%
  mutate(period = case_when(schoolyr %in% pre_program_schoolyrs ~ 0,
                            schoolyr %in% LAIV_schoolyrs ~ 1,
                            schoolyr %in% IIV_schoolyrs ~ 2)) %>%
  filter(schoolyr != "2017-18")
\end{verbatim}

  And of a complex \texttt{ggplot} call:

\begin{verbatim}
# Example 3
ggplot(data=data,
       mapping=aes_string(x="year", y="rd", group=group)) +

  geom_point(mapping=aes_string(col=group, shape=group),
             position=position_dodge(width=0.2),
             size=2.5) +

  geom_errorbar(mapping=aes_string(ymin="lb", ymax="ub", col=group),
                position=position_dodge(width=0.2),
                width=0.2) +

  geom_point(position=position_dodge(width=0.2),
             size=2.5) +

  geom_errorbar(mapping=aes(ymin=lb, ymax=ub),
                position=position_dodge(width=0.2),
                width=0.1) +

  scale_y_continuous(limits=limits,
                     breaks=breaks,
                     labels=breaks) +

  scale_color_manual(std_legend_title,values=cols,labels=legend_label) +
  scale_shape_manual(std_legend_title,values=shapes, labels=legend_label) +
  geom_hline(yintercept=0, linetype="dashed") +
  xlab("Program year") +
  ylab(yaxis_lab) +
  theme_complete_bw() +
  theme(strip.text.x = element_text(size = 14),
        axis.text.x = element_text(size = 12)) +
  ggtitle(title)
\end{verbatim}
\end{itemize}

Imagine (or perhaps mournfully recall) the mess that can occur when you don't strictly style a complicated \texttt{ggplot} call. Trying to fix bugs and ensure your code is working can be a nightmare. Now imagine trying to do it with the same code 6 months after you've written it. Invest the time now and reap the rewards as the code practically explains itself, line by line.

\hypertarget{automated-tools-for-style-and-project-workflow}{%
\section{Automated Tools for Style and Project Workflow}\label{automated-tools-for-style-and-project-workflow}}

\hypertarget{styling}{%
\subsection{Styling}\label{styling}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Code Autoformatting} - RStudio includes a fantastic built-in utility (keyboard shortcut: \texttt{CMD-Shift-A}) for autoformatting highlighted chunks of code to fit many of the best practices listed here. It generally makes code more readable and fixes a lot of the small things you may not feel like fixing yourself. Try it out as a ``first pass'' on some code of yours that \emph{doesn't} follow many of these best practices!
\item
  \textbf{Assignment Aligner} - A \href{https://www.r-bloggers.com/align-assign-rstudio-addin-to-align-assignment-operators/}{cool R package} allows you to very powerfully format large chunks of assignment code to be much cleaner and much more readable. Follow the linked instructions and create a keyboard shortcut of your choosing (recommendation: \texttt{CMD-Shift-Z}). Here is an example of how assignment aligning can dramatically improve code readability:
\end{enumerate}

\begin{verbatim}
# Before
OUSD_not_found_aliases <- list(
  "Brookfield Village Elementary" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Brookfield"),
  "Carl Munck Elementary" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Munck"),
  "Community United Elementary School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Community United"),
  "East Oakland PRIDE Elementary" = str_subset(string = OUSD_school_shapes$schnam, pattern = "East Oakland Pride"),
  "EnCompass Academy" = str_subset(string = OUSD_school_shapes$schnam, pattern = "EnCompass"),
  "Global Family School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Global"),
  "International Community School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "International Community"),
  "Madison Park Lower Campus" = "Madison Park Academy TK-5",
  "Manzanita Community School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Manzanita Community"),
  "Martin Luther King Jr Elementary" = str_subset(string = OUSD_school_shapes$schnam, pattern = "King"),
  "PLACE @ Prescott" = "Preparatory Literary Academy of Cultural Excellence",
  "RISE Community School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Rise Community")
)
\end{verbatim}

\begin{verbatim}
# After
OUSD_not_found_aliases <- list(
  "Brookfield Village Elementary"      = str_subset(string = OUSD_school_shapes$schnam, pattern = "Brookfield"),
  "Carl Munck Elementary"              = str_subset(string = OUSD_school_shapes$schnam, pattern = "Munck"),
  "Community United Elementary School" = str_subset(string = OUSD_school_shapes$schnam, pattern = "Community United"),
  "East Oakland PRIDE Elementary"      = str_subset(string = OUSD_school_shapes$schnam, pattern = "East Oakland Pride"),
  "EnCompass Academy"                  = str_subset(string = OUSD_school_shapes$schnam, pattern = "EnCompass"),
  "Global Family School"               = str_subset(string = OUSD_school_shapes$schnam, pattern = "Global"),
  "International Community School"     = str_subset(string = OUSD_school_shapes$schnam, pattern = "International Community"),
  "Madison Park Lower Campus"          = "Madison Park Academy TK-5",
  "Manzanita Community School"         = str_subset(string = OUSD_school_shapes$schnam, pattern = "Manzanita Community"),
  "Martin Luther King Jr Elementary"   = str_subset(string = OUSD_school_shapes$schnam, pattern = "King"),
  "PLACE @ Prescott"                   = "Preparatory Literary Academy of Cultural Excellence",
  "RISE Community School"              = str_subset(string = OUSD_school_shapes$schnam, pattern = "Rise Community")
)
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  \textbf{StyleR} - Another \href{https://www.tidyverse.org/articles/2017/12/styler-1.0.0/}{cool R package from the Tidyverse} that can be powerful and used as a first pass on entire projects that need refactoring. The most useful function of the package is the \texttt{style\_dir} function, which will style all files within a given directory. See the \href{https://www.rdocumentation.org/packages/styler/versions/1.1.0/topics/style_dir}{function's documentation} and the vignette linked above for more details.

  \begin{itemize}
  \tightlist
  \item
    \textbf{Note}: The default Tidyverse styler is subtly different from some of the things we've advocated for in this document. Most notably we differ with regards to the number of spaces before/after ``tokens'' (i.e.~Assignment Aligner add spaces before \texttt{=} signs to align them properly). For this reason, we'd recommend the following: \texttt{style\_dir(path\ =\ ...,\ scope\ =\ "line\_breaks",\ strict\ =\ FALSE)}. You can also customize StyleR \href{http://styler.r-lib.org/articles/customizing_styler.html}{even more} if you're really hardcore.
  \item
    \textbf{Note}: As is mentioned in the package vignette linked above, StyleR modifies things \emph{in-place}, meaning it overwrites your existing code and replaces it with the updated, properly styled code. This makes it a good fit on projects \emph{with version control}, but if you don't have backups or a good way to revert back to the intial code, I wouldn't recommend going this route.
  \end{itemize}
\end{enumerate}

\hypertarget{datamanagement}{%
\chapter{Data Management}\label{datamanagement}}

\emph{Contributors: Kunal Mishra, Jade Benjamin-Chung, Ben Arnold}

2019-10-10: THIS CHAPTER IS A WORK IN PROGRESS (INCOMPLETE)

\hypertarget{data-inputoutput-io}{%
\section{Data input/output (I/O)}\label{data-inputoutput-io}}

\hypertarget{rds-vs-.rdata-files}{%
\subsection{\texorpdfstring{\texttt{.RDS} vs \texttt{.RData} Files}{.RDS vs .RData Files}}\label{rds-vs-.rdata-files}}

One of the most common ways to load and save data in Base R is with the \texttt{load()} and \texttt{save()} functions to serialize multiple objects in a single \texttt{.RData} file. The biggest problems with this practice include an inability to control the names of things getting loaded in, the inherent confusion this creates in understanding older code, and the inability to load individual elements of a saved file. For this, we recommend using the RDS format to save R objects using \texttt{saveRDS()} and its complement \texttt{readRDS()}.

\begin{itemize}
\item
  \textbf{Note}: if you have many related R objects you would have otherwise saved all together using the \texttt{save} function, the functional equivalent with \texttt{RDS} would be to create a (named) list containing each of these objects, and saving it.
\item
  \textbf{Note}: there is an important caveat for \texttt{.rds} files: they are not automatically backward compatible across different versions of R! So, while they are very useful in general, beware. See, for example, this thread on \href{https://stackoverflow.com/questions/56704638/write-a-file-using-saverds-so-that-it-is-backwards-compatible-with-old-versi}{StackExchange}. \texttt{.csv} files embed slightly less information (typically), but are more stable across different versions of R.
\end{itemize}

\hypertarget{csv-files}{%
\subsection{\texorpdfstring{\texttt{.CSV} Files}{.CSV Files}}\label{csv-files}}

Once again, the \texttt{readr} package as part of the Tidvyerse is great, with a much faster \texttt{read\_csv()} than Base R's \texttt{read.csv()}. For massive CSVs (\textgreater{} 5 GB), you'll find \texttt{data.table::fread()} to be the fastest CSV reader in any data science language out there. For writing CSVs, \texttt{readr::write\_csv()} and \texttt{data.table::fwrite()} outclass Base R's \texttt{write.csv()} by a significant margin as well.

\hypertarget{publishing-public-data}{%
\subsection{Publishing public data}\label{publishing-public-data}}

\textbf{NEVER} push a dataset into the public domain (e.g., GitHub, OSF) without first checking with Ben to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so.

If you are releasing data into the public domain, then consider making available \emph{at minimum} a \texttt{.csv} file and a codebook of the same name (note: you should have a codebook for internal data as well). We often also make available \texttt{.rds} files as well. For example, your \texttt{mystudy/data/public} directory could include three files for a single dataset, two with the actual data in \texttt{.rds} and \texttt{.csv} formats, and a third that describes their contents:

\begin{verbatim}
analysis_data_public.csv
analysis_data_public.rds
analysis_data_public_codebook.txt
\end{verbatim}

In general, datasets are usually too big to save on GitHub, but occasionally they are small. Here is an example of where we actually pushed the data directly to GitHub: \url{https://github.com/ben-arnold/enterics-seroepi/tree/master/data} .

If the data are bigger, then maintaining them under version control in your git repository can be unweildy. Instead, we recommend using another stable repository that has version control, such as the Open Science Framework (\href{https://osf.io}{osf.io}). For example, all of the data from the WASH Benefits trials (led by investigators at Berkeley, icddr,b, IPA-Kenya and others) are all stored through data components nested within in OSF projects: \url{https://osf.io/tprw2/}.

\hypertarget{documenting-datasets}{%
\section{Documenting datasets}\label{documenting-datasets}}

Datasets need to have metadata (documentation) associated with them to help people understand them. Well documented datasets save an enormous amount of time because it helps avoid lots of back-and-forth with new people orienting themselves with the data. This applies to both private and public data used in your work flow.

Each final dataset should include a codebook. The file \href{https://github.com/ben-arnold/enterics-seroepi/blob/master/data/asembo_analysis_codebook.txt}{asembo\_analysis\_codebook.txt} provides one example of what a codebook for a simple dataset could contain.

For complex studies with multiple, relational data files, it is exceptionally helpful to also include a README overview in plain text or markdown that explains the relationships between the datasets. Here is an example from the WASH Benefits Bangladesh trial primary outcomes analysis: \href{https://osf.io/v3nfs/}{README-WBB-primary-outcomes-datasets.md}.

\hypertarget{github}{%
\chapter{GitHub and Version Control}\label{github}}

\emph{Contributors: Stephanie Djajadi, Nolan Pokpongkiat, and Ben Arnold}

\hypertarget{basics}{%
\section{Basics}\label{basics}}

Git is a version control system. It has very good documentation online: \url{https://git-scm.com/doc}

If you get into trouble with Git, the docs will help a lot!

Git is often used in conjunction with GitHub, which is an online platform to help teams collaborate while using Git for version control: \url{https://github.com}.

\begin{itemize}
\tightlist
\item
  A detailed tutorial of Git can be found \href{https://sp19.datastructur.es/materials/guides/using-git\#b-local-repositories-narrative-introduction}{here on UC Berkeley's CS61B website}.
\item
  If you are already familiar with Git, you can reference the summary at the end of \href{https://sp19.datastructur.es/materials/guides/using-git\#b-local-repositories-narrative-introduction}{Section B}.
\item
  If you have made a mistake in Git, you can refer to this \href{https://sethrobertson.github.io/GitFixUm/fixup.html}{article} to undo, fix, or remove commits in git.
\end{itemize}

\hypertarget{git-branching}{%
\section{Git Branching}\label{git-branching}}

Branches allow you to keep track of multiple versions of your work simultaneously, and you can easily switch between versions and merge branches together once you've finished working on a section and want it to join the rest of your code. Here are some cases when it may be a good idea to branch:

\begin{itemize}
\tightlist
\item
  You may want to make a dramatic change to your existing code (called refactoring) but it will break other parts of your project. But you want to be able to simultaneously work on other parts or you are collaborating with others, and you don't want to break the code for them.
\item
  You want to start working on a new part of the project, but you aren't sure yet if your changes will work and make it to the final product.
\item
  You are working with others and don't want to mix up your current work with theirs, even if you want to bring your work together later in the future.
\end{itemize}

A detailed tutorial on Git Branching can be found \href{https://sp19.datastructur.es/materials/guides/using-git\#e-git-branching-advanced-git-optional}{here}. You can also find instructions on how to handle merge conflicts when joining branches together.

\hypertarget{example-workflow}{%
\section{Example Workflow}\label{example-workflow}}

A terrific overview of branching workflow is here: \url{https://guides.github.com/introduction/flow/}

A standard workflow when starting on a new project and contributing code looks like this:

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright
Command\strut
\end{minipage} & \begin{minipage}[b]{0.60\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.34\columnwidth}\raggedright
SETUP: FIRST TIME ONLY: \texttt{git\ clone\ \textless{}url\textgreater{}\ \textless{}directory\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
Clone the repo. This copies of all the project files in its current state on Github to your local computer.\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
1. \texttt{git\ pull\ origin\ master}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
update the state of your files to match the most current version on GitHub\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
2. \texttt{git\ checkout\ -b\ \textless{}new\_branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
create new branch that you'll be working on and go to it\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
3. Make some file changes\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
work on your feature/implementation\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
4. \texttt{git\ add\ \textless{}filename\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
add file to stage for commit\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
5. \texttt{git\ commit\ -m\ \textless{}commit\ message\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
commit file with a message\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
6. \texttt{git\ push\ -u\ origin\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
push branch to remote and set to track (-u only works if this is first push)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
7. Repeat step 4-5.\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
work and commit often\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
8. \texttt{git\ push}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
push work to remote branch for others to view\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
9. Follow the link given from the \texttt{git\ push} command to submit a pull request (PR) on GitHub online\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
PR merges in work from your branch into master\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
(10.) Your changes and PR get approved, your reviewer deletes your remote branch upon merging\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
11. \texttt{git\ fetch\ -\/-all\ -\/-prune}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
clean up your local git by untracking deleted remote branches\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

Other helpful commands are listed below.

\hypertarget{commonly-used-git-commands}{%
\section{Commonly Used Git Commands}\label{commonly-used-git-commands}}

\begin{longtable}[]{@{}ll@{}}
\toprule
\begin{minipage}[b]{0.34\columnwidth}\raggedright
Command\strut
\end{minipage} & \begin{minipage}[b]{0.60\columnwidth}\raggedright
Description\strut
\end{minipage}\tabularnewline
\midrule
\endhead
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ clone\ \textless{}url\textgreater{}\ \textless{}directory\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
clone a repository, only needs to be done the first time\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ pull\ origin\ master}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
pull before making any changes\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ branch}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
check what branch you are on\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ branch\ -a}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
check what branch you are on + all remote branches\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ checkout\ -b\ \textless{}new\_branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
create new branch and go to it (only necessary when you create a new branch)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ checkout\ \textless{}branch\ name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
switch to branch\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ add\ \textless{}file\ name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
add file to stage for commit\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ commit\ -m\ \textless{}commit\ message\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
commit file with a message\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ push\ -u\ origin\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
push branch to remote and set to track (-u only works if this is first push)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ branch\ -\/-set-upstream-to\ origin\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
set upstream to origin/ (use if you forgot -u on first push)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ push\ origin\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
push work to branch\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ checkout\ -\/-track\ origin/\textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
pulls a remote branch and creates a local branch to track it (use when trying to pull someone else's branch onto your local computer)\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ push\ -\/-delete\ \textless{}remote\_name\textgreater{}\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
delete remote branch\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ branch\ -d\ \textless{}branch\_name\textgreater{}}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
deletes local branch, -D to force\strut
\end{minipage}\tabularnewline
\begin{minipage}[t]{0.34\columnwidth}\raggedright
\texttt{git\ fetch\ -\/-all\ -\/-prune}\strut
\end{minipage} & \begin{minipage}[t]{0.60\columnwidth}\raggedright
untrack deleted remote branches\strut
\end{minipage}\tabularnewline
\bottomrule
\end{longtable}

\hypertarget{how-often-should-i-commit}{%
\section{How often should I commit?}\label{how-often-should-i-commit}}

Stephanie and Nolan (trained in CS and Data Science) suggest it is good practice to commit every 15 minutes (a time-based guideline), or every time you make a significant change (progress-based guideline). Ben's perspective aligns with this view, but is weighted toward committing around completion of discrete chunks of work; for him, a discrete chunk of work will often take quite a bit longer than 15 minutes time. Take home message: \emph{It is better to commit more rather than less.}

\hypertarget{what-should-be-pushed-to-github}{%
\section{What should be pushed to Github?}\label{what-should-be-pushed-to-github}}

In general, it is better to track text-based files (\texttt{.R}, \texttt{.Rmd}, \texttt{.md},\texttt{.txt},etc\ldots) compared with binary files (\texttt{.pdf},\texttt{.png},\texttt{.docx}, etc\ldots) because Git will store changes to a binary file as a completely new file in your Git directory. If you store 100 versions of the same binary file, your directory will quickly become very bloated. If the binary files don't change often, then you could consider including them under version control, but it is usually cleaner to keep them under a separate version control, such as through an Open Science Framework project with a specific data component.

Be careful before you push .Rout log files! If someone else runs an R script and creates an .Rout file at the same time and both of you try to push to github, it is incredibly difficult to reconcile these two logs. If you run logs, keep them on your own system or (preferably) set up a shared directory where all logs are name and date timestamped.

There is a standardized \texttt{.gitignore} for \texttt{R} which you \href{https://github.com/github/gitignore/blob/master/R.gitignore}{can download} and add to your project. This ensures you're not committing log files or things that would otherwise best be left ignored to GitHub. This is a \href{https://www.tidyverse.org/articles/2017/12/workflow-vs-script/}{great discussion of project-oriented workflows}, extolling the virtues of a self-contained, portable projects, for your reference.

\hypertarget{how-should-i-describe-my-commit}{%
\section{How should I describe my commit?}\label{how-should-i-describe-my-commit}}

When you commit, always include a short commit message that describes what the commit does. In the command line, you can achieve this after you have staged files to commit with the \texttt{commit\ -m\ \textless{}"your\ commit\ message\ here"\textgreater{}} syntax. This helps track-back through work flow. For example, if the commit message is \texttt{new}, that doesn't provide any information about what the commit includes. A more descriptive commit message would be \texttt{Create\ first\ draft\ of\ Fig\ 1\ distribution\ plot} or \texttt{Change\ color\ scheme\ for\ distribution\ plot}.

For more lengthy and detailed commit messages, which go beyond the simple, single line \texttt{commit\ -m\ \textless{}your\ commit\ message\ here\textgreater{}} synatax, this \href{https://medium.com/@andrewhowdencom/anatomy-of-a-good-commit-message-acd9c4490437}{Medium post} on the anatomy of a good commit message includes additional discussion. (Note, we don't typically use commits that are this detailed!)

\hypertarget{bigdata}{%
\chapter{Working with Big Data}\label{bigdata}}

TBD

\hypertarget{unix}{%
\chapter{UNIX Commands}\label{unix}}

TBD

\hypertarget{commcoord}{%
\chapter{Communication and Coordination}\label{commcoord}}

\emph{Contributors: Jade Benjamin-Chung, Ben Arnold}

\textbf{These communications guidelines are evolving as we increasingly adopt Slack, but here some general principles if you work closely with Ben.}

\hypertarget{slack}{%
\section{Slack}\label{slack}}

\begin{itemize}
\item
  If you work with Ben but are not a member of Ben's Slack workspace then ask him to invite you!
\item
  Use Slack for scheduling, coding related questions, quick check ins, etc. If your Slack message exceeds 200 words, it might be time to use email.
\item
  Use channels instead of direct messages unless you need to discuss something private.
\item
  Include tags on your message (e.g., \texttt{@Ben}) when you want to ensure that a person sees the message. Ben doesn't regularly read messages where he isn't tagged.
\item
  Please make an effort to respond to messages that message you (e.g., \texttt{@Ben}) as quickly as possible and always within 24 hours, unless of course you are on vacation!
\item
  If you are unusually busy (e.g., taking MCAT/GRE, taking many exams) or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual and also \href{https://get.slack.help/hc/en-us/articles/201864558-Set-your-Slack-status-and-availability}{set your status in Slack} (e.g., it could say ``On vacation'') so we know not to expect to see you online.
\item
  Please thread messages in Slack as much as possible.
\end{itemize}

\hypertarget{email}{%
\section{Email}\label{email}}

\begin{itemize}
\tightlist
\item
  Use email for longer messages (\textgreater200 words) or messages that merit preservation.
\item
  Generally, strive to respond within 24 hours. If you are unusually busy or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual.
\end{itemize}

\hypertarget{trello}{%
\section{Trello}\label{trello}}

\begin{itemize}
\tightlist
\item
  Ben manages projects and teams using a \href{https://www.atlassian.com/agile/kanban/boards}{kanban board approach} in \href{https://www.trello.com}{Trello}.
\item
  You and/or Ben will add new cards within our team's Trello boards and assign them to team members.
\item
  Each card represents a discrete chunk of work.
\item
  Cards higher in a list are higher priority.
\item
  Strive to complete the tasks in your card by the card's due date. Talk to Ben about deadlines -- we can always manage the calendar!
\item
  Use checklists to break down a task into smaller chunks. Usually, you can do this yourself (but ask Ben if you ever want input).
\item
  Move cards to the ``DONE'' list on a board when they are done.
\end{itemize}

\hypertarget{google-drive}{%
\section{Google Drive}\label{google-drive}}

\begin{itemize}
\tightlist
\item
  We mostly use Google Drive to create shared documents with longer descriptions of tasks. These documents are linked to Trello cards. Ben often shares these docs with a whole project team since tasks are overlapping, and even if a task is assigned to one person, others may have valuable insights.
\item
  Please invite both of Ben's email addresses to any documents you create (\href{mailto:bfarnold@gmail.com}{\nolinkurl{bfarnold@gmail.com}}, \href{mailto:ben.arnold@ucsf.edu}{\nolinkurl{ben.arnold@ucsf.edu}}).
\end{itemize}

\hypertarget{calendar-meetings}{%
\section{Calendar / Meetings}\label{calendar-meetings}}

\begin{itemize}
\tightlist
\item
  Ben will schedule most meetings through the calendar.
\item
  Our meetings start on the hour.
\item
  If you are going to be late, please send a message in our Slack channel.
\item
  If you are regularly not able to come on the hour, notify the team and we might choose the modify the agenda order or the start time.
\item
  Add hoc meetings are welcome. If Ben's office door is open, come in!
\end{itemize}

\hypertarget{code-of-conduct}{%
\chapter{Code of conduct}\label{code-of-conduct}}

\emph{Contributors: Jade Benjamin-Chung, Ben Arnold}

\hypertarget{group-culture}{%
\section{Group culture}\label{group-culture}}

We strive to work in an environment that is collaborative, supportive, open, and free from discrimination and harassment, per University policies.

We encourage students / staff of all experience levels to respectfully share their honest opinions and ideas on any topic. Our group has thrived upon such respectful honest input from team members over the years, and this document is a product of years of student and staff input (and even debate) that has gradually improved our productivity and overall quality of our work.

If Ben is your PI, be forewarned that he tends to batch his email communication (\textasciitilde30 mins in the morning and afternoon, 15 mins mid-day), and doesn't tend to answer Slack or email during evenings or weekends. If you need to reach him urgently then give him a call or text on his mobile.

\hypertarget{protecting-human-subjects}{%
\section{Protecting human subjects}\label{protecting-human-subjects}}

All lab members must complete \href{https://irb.ucsf.edu/citi-human-subjects-training}{CITI Human Subjects Training} and share their certificate with Ben. We will will add team members to relevant Institutional Review Board protocols to ensure they have permission to work with identifiable datasets.

One of the most relevant aspects of protecting human subjects in our work in the Data Coordinating Center is maintaining confidentiality and data privacy. For students supporting our data science efforts, in practice this means:

\begin{itemize}
\tightlist
\item
  If you are using a virtual computer (e.g., Google Cloud, AWS, Optum), never save the data in that system to your personal computer or any other computer without prior permission.
\item
  Do not share data with anyone without first obtaining permission, including to other members of the Proctor Foundation, who might not be on the same IRB protocol as you (check with Ben or the relevant PI first).
\item
  \textbf{NEVER} push a dataset into the public domain (e.g., GitHub, OSF) without first checking with Ben to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so.
\end{itemize}

Remember, data that looks like it does not contain identifiers to you might still be classified as data that requires special protection by our IRB or under HIPAA, so always proceed with caution and ask for help if you have any concerns about how to maintain study participant confidentiality. For example, the combination of age, sex, and geographic location of the individual's town or neighborhood is typically considered identifiable.

\hypertarget{authorship}{%
\section{Authorship}\label{authorship}}

We adhere to the \href{http://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-role-of-authors-and-contributors.html}{ICMJE Definition of authorship} and are happy for team members who meet the definition of authorship to be included as co-authors on scientific manuscripts.

\hypertarget{work-hours}{%
\section{Work hours}\label{work-hours}}

Please follow the Proctor Foundation's employee guidelines for work hours, and discuss the specifics with your PI. If Ben is your PI, then work with him on your schedule to ensure we have overlap in the office and that you are around at key times for group meetings, etc.

\hypertarget{resources}{%
\chapter{Additional Resources}\label{resources}}

TBD

\bibliography{book.bib,packages.bib}


\end{document}
