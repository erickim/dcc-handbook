[
["index.html", "Proctor Foundation Data Science Handbook Welcome!", " Proctor Foundation Data Science Handbook Contributors (many from UC Berkeley in addition to Proctor): Ben Arnold, Jade Benjamin-Chung, Kunal Mishra, Anna Nguyen, Nolan Pokpongkiat, Stephanie Djajadi 2019-10-08 Welcome! Welcome to the Francis I. Proctor Foundation at the University of California, San Francisco (https://proctor.ucsf.edu)! This handbook summarizes some best practices for data science, drawing from our experience at the Francis I. Proctor Foundation and from that of our close colleagues in the Division of Epidemiology and Biostatistics at the University of California, Berkeley (where Prof. Ben Arnold worked for many years before joining Proctor). We do not intend this handbook to be a comprehensive guide to data science. Instead, it focuses more on practical, “how-to” guidance for conducting data science within epidemiologic research studies. Although many of the ideas of environment-independent, the examples draw from the R programming language. For an excellent overview of data science in R, see the book R for Data Science. Much of the material in this handbook evolved from a version of Dr. Jade Benjamin-Chung’s lab manual at the University of California, Berkeley. In addition to the Proctor team, many contributors include current and former students from UC Berkeley. The last two chapters of the handbook cover our communication strategy and code of conduct for team members who work with Prof. Ben Arnold, who leads Proctor’s Data Coordinating Center. They summarize key pieces of a functional data science team. Although the last two chapters might be of interest to a broader circle, they are mostly relevant for people working directly with Ben. Just because they are at the end does not make them less important. It is a living document that we strive to update regularly. If you would like to contribute, please write Ben (ben.arnold@ucsf.edu) and/or submit a pull request. The GitHub repository for this handbook is: https://github.com/proctor-ucsf/dcc-handbook "],
["intro.html", "Chapter 1 Introduction: Work Flow and Reproducible Analyses 1.1 Workflow 1.2 Reproducibility 1.3 Automation", " Chapter 1 Introduction: Work Flow and Reproducible Analyses Contributors: Ben Arnold This handbook collates a number of tips to help organize the workflow of epidemiologic data analyses. There are probably a dozen good ways to organize a workflow for reproducible research. This document includes recommendations that arise from our own team’s experience through numerous field trials and observational data analyses. The recommendations will not work for everybody or for all applications. But, they work well for most of us most of the time, else we wouldn’t put in the time to share them. Start with two organizing concepts: Workflow. Defined here as the process required to draw scientific inference from data collected in the field or lab. I.e., the process by which we take data, and then process it, share it internally, analyze it, and communicate results to the scientific community. Reproducible research. A fundamental characteristic of the scientific method is that study findings can be reproduced beyond the original investigators. Data analyses that contribute to scientific research should be described and organized in a way that they could be reproduced by an independent person or research group. A data analysis that is not reproducible violates a core principle of the scientific method. 1.1 Workflow Broadly speaking, a typical scientific data science work flow involves four steps to transform raw data (e.g., from the field) into summaries that communicate results to the scientific community. Figure 1.1: Overview of the four main steps in a typical data science workflow When starting a new project, the work flow tends to evolve gradually and by iteration. Data cleaning, data processing, exploratory analyses, back to data cleaning, and so forth. If the work takes place in an unstructured environment with no system to organize files and work flow, it rapidly devolves into into a disorganized mess; analyses become difficult or impossible to replicate and they are anything but scientific. Projects with short deadlines (e.g., proposals, conference abstract submissions, article revisions) are particularly vulnerable to this type of organizational entropy. Putting together a directory and workflow plan from the start helps keep files organized and prevent disorder. Modifications are inevitable – as long as the system is organized, modifications are usually no problem. Depending on the project, each step involves a different amount of work. Step 1 is by far the most time consuming, and often the most error-prone. We devote an entire chapter to it below (Data cleaning and processing) 1.2 Reproducibility As a guiding directive, this process should be reproducible. If you are not familiar with the concept of reproducible research, start with this manifesto (Munafo et al. 2017). For a deeper dive, we highly recommend the recent book from Christensen, Freese, and Miguel (2019). Although it is framed around social science, the ideas apply generally. 1.3 Automation We recommend that the workflow be as automated as possible using a programming language. Automating the workflow in a programming language, and essentially reducing it to text, is advantageous because it makes the process transparent, well documented, easily modified, and amenable to version control; these characteristics lend themselves to reproducible research. At Proctor, we mostly use R. With the development of Rstudio, R Markdown and the tidyverse ecosystem (among others), the R language has evolved as much in the past few years as in all previous decades since its inception. This has made the conduct of automated, reproducible research considerably easier than it was 10 years ago. If you have a step in your analysis workflow that involves point-and-click or copy/paste, then STOP, and ask yourself (and your team): How can I automate this? "],
["workflows.html", "Chapter 2 Workflows", " Chapter 2 Workflows Contributors: Ben Arnold A data science work flow typically progresses through 4 steps that rarely evolve in a purely linear fashion, but in the end should flow in this direction: Figure 2.1: Overview of the four main steps in a typical data science workflow Table 2.1: Workflow basics Steps Example activities \\(\\Rightarrow\\) Inputs \\(\\Rightarrow\\) Outputs 1 Data cleaning and processing . make a plan for final datasets, fix data entry errors, create derived variables, plan for public replication files untouched datasets final datasets 2-3 Analyses . exploratory data analysis, study monitoring, summary statistics, statistical analyses, independent replication of analyses, make figures and tables final datasets saved results (.rds/.csv), tables (.html,.pdf), figures (.html/.png) 4 Communication . results synthesis saved results, figures, tables monitoring reports, presentations, scientific articles In many modern data science workflows, steps 2-4 can be accomplished in a single R notebook or Jupyter notebook: the statistical analysis, creation of figures and tables, and creation of reports. However, it is still useful to think of the distinct stages in many cases. For example, a single statistical analysis might contribute to a DSMC report, a scientific conference presentation, and a scientific article. In this example, each piece of scientific communication would take the same input (stored analysis results as .csv/.rds) and then proceed along slightly different downstream workflows. It would be more error prone to replicate the same statistical analysis in three parallel downstream work flows. This illustrates a key idea that holds more generally: Key idea for workflows: Whenever possible, avoid repeating the same data processing or statistical analysis in separate streams. Key data processing and analyses should be done once. "],
["directory-structure-and-code-repositories.html", "Chapter 3 Directory Structure and Code Repositories 3.1 Small and large projects 3.2 Directory Structure 3.3 Code Repositories", " Chapter 3 Directory Structure and Code Repositories Contributors: Kunal Mishra, Jade Benjamin-Chung, Ben Arnold The backbone of your project workflow is the file directory so it makes sense to spend time organizing the directory. Note that directory is the technical term for the system used to organize individual files. Most non-UNIX environments use a folder analogy, and directory and folder can be used interchangeably in a lot of cases. A well organized directory will make everything that follows much easier. Just like a well designed kitchen is essential to enjoy cooking (and avoid clutter), a well designed directory helps you enjoy working and stay organized in a complex project with literally thousands of related files. Just like a disorganized kitchen (“now where did I put that spatula?”) a disorganized project directory creates confusion, lost time, stress, and mistakes. Another huge advantage of maintaining a regular/predictable directory structure within a project and across projects is that it makes it more intuitive. When a directory is intuitive, it is easier to work collaboratively across a larger team; everybody can predict (at least approximately) where files should be. Nested within your directory will be a code repository. Sometimes we find it useful to manage the code repository using version control, such as git/GitHub. Other chapters will discuss coding practices, data management, and GitHub/version control that will build from the material here. Carrying the kitchen analogy further: here, we are designing the kitchen. Then, we’ll discuss approaches for how to cook in the kitchen that we designed/built. 3.1 Small and large projects Our experience is that the overwhelming majority of projects come in two sizes: small and large. We recommend setting up your directory structure depending on how large you expect the project to be. Sometimes, small projects evolve into large projects, but only occasionally. A small project is something like a single data analysis with a single published article in mind. A large project is an epidemiologic field study, where there are multiple different types of data and different types of analyses (e.g., sample size calculations, survey data, biospecimens, substudies, etc.). Small project: There is essentially one dataset and a single, coherent analysis. For example, a simulation study or a methodology study that will lead to a single article. Large project: A field study that includes multiple activities, each of which generates data files. Multiple analyses are envisioned, leading to multiple scientific articles. Large projects are more common and more complicated. Most of this chapter focuses on large project organization (small projects can be thought of as essentially one piece of a large project). 3.2 Directory Structure In the example below, we follow a basic directory naming convention that makes working in UNIX and typing directory statements in programs much easier: short names no spaces in the names (not essential but a personal preference. Can use _ or - instead) lower case (not essential, again, personal preferences vary!) For example, Ben completed a study in Tamil Nadu, India during his dissertation to study the effect of improvements in water supply and sanitation on child health. Instead of naming the directory Tamil Nadu or Tamil Nadu WASH Study, he used trichy instead (a colloquial name for the city near the study, Tiruchirappalli), which was much easier to type in the terminal and in directory statements. A short name helps make directory references easier while programming. 3.2.1 First level: data and analyses Figure 3.1: Example directory for mystudy Start by dividing a project into major activities. In the example abpve, the project is named mystudy. There is a data subdirectory (more in a sec), and then three major activities, each corresponding to a separate analysis: primary-analysis,secondary-analysis-1, and secondary-analysis-2. In a real project, the names could be more informative, such as “trachoma-qpcr”. Also, a real project might also include many additional subdirectories related to administrative and logistics activities that do not relate to data science, such as irb, travel, contracts, budget, survey forms, etc.). Dividing files into major activities helps keep things organized for really big projects. In a multi-site study, consider including a directory for each site before splitting files into major activities. Ideally, analyses will not span major activity subdirectories in a project folder, but sometimes you can’t predict/avoid that from happening. 3.2.2 Second level: data Each project will include a data directory. We recommend organizing it into 3 parts: untouched, temp, and final. Often, it is useful to include a fourth subdirectory called public for sharing public versions of datasets. The untouched directory includes all untouched datasets that are used for the study. Once saved in the directory never touch them; you will read them into the work flow, but never, ever save over them. If the study has repeated extracts from rolling data collection or electronic health records, consider subdirectories within untouched that are indexed by date. The temp directory (optional, not essential) includes temporary files that you might generate during the data management process. This is mainly a space for experimentation. As a rule, never save anything in the temp directory that you cannot delete. Regularly delete files in the temp directory to save disk space. The final directory includes final datasets for the activity. Final datasets are de-identified and require no further processing; they are clean and ready for analysis. They should be accompanied by meta-data, which at minimum includes the data’s provenance (i.e., how it was created) and what it includes (i.e., level of the data, plus variable coding/labels/descriptions). Clean/final datasets generated by one analysis might be reused in another. 3.2.3 Second level: analysis We recommend maintaining a separate subdirectory for each major analysis in a project. In this example, there are three with not-very-creative names from the view of trial: primary-analysis, secondary-analysis-1, secondary-analysis-2. Think of each analysis as the scope of all of the work for a single, published paper. We recommend dividing the analysis project into a space for computational notebooks / scripts (i.e., a code repository), and a second for their output. The reason for the split is to make it easier to use version control (should you choose) for the code. Version control like git and GitHub (see the Chapter on GitHub) works well for text files but isn’t really designed for binary files such as images (.png), datasets (.rds), or PDF files (.pdf). It is certainly possible to use git with those file types, but since git makes a new copy of the file every time it is changed the git repo can get horribly bloated and takes up too much space on disk. Consolidating the output into a separate directory makes it more obvious that it isn’t under version control. In this example, there are separate parts for code (R) and output (output). Output could include figures, tables, or saved analysis results stored as data files (.rds or .csv). Another conventional name for the code repository is src as an alternative to R if you use other languages. Interdependence between analyses: Sometimes a result from an analysis might be a cleaned dataset that could feed into future, distinct analyses. This is quite common, for example, in large trials where a set of baseline characteristics might be used in multiple separate papers for different endpoints, either for assessing balance of the trial population or subgroups, or used as adjustment covariates in additional analyses of the trial. In this case, the cleaned dataset would be written to the data/final directory and is thus available for future use. 3.3 Code Repositories Maintain a separate code repository for each major analysis activity (last section). We recommend the following structure for a code repository: With subdirectories that generally look like this: .gitignore primary-analysis.Rproj 0-config.R 0-primary-analysis-run-all.sh 1-dm / 0-dm-run-all.sh 1-format-enrollment-data.R 2-format-adherence-data.R 3-format-LAZ-measurements.R 2-baseline-balance / 0-baseline-balance-run-all.sh ... 3-consort / 0-consort-run-all.sh ... 4-adherence / 0-adherence-run-all.sh ... 5-laz / 0-laz-run-all.sh 1-laz-unadjusted-analysis.R 2-laz-adjusted-analysis.R 6-figures / 0-figures-run-all.sh Fig1-consort.Rmd Fig2-adherence.Rmd Fig3-laz.Rmd Note dm is shorthand for “data management.” You can call the data management directory anything you want, but just ensure that you have one. This helps ensure work conducted in step 1 of your workflow stays upstream from all analyses (see Chapter on workflows). Also note that in this example, all of the scripts are .R files. Increasingly, we use R Markdown notebooks .Rmd instead of / in addition to R files. For brevity, we haven’t expanded every directory, but you can glean some important takeaways from what you do see. 3.3.1 .Rproj files An “R Project” can be created within RStudio by going to File &gt;&gt; New Project. Depending on where you are with your research, choose the most appropriate option. This will save preferences, working directories, and even the results of running code/data (though we recommend starting from scratch each time you open your project, in general). Then, ensure that whenever you are working on that specific research project, you open your created project to enable the full utility of .Rproj files. This also automatically sets the directory to the top level of the project. 3.3.2 Configuration (‘config’) File This is the single most important file for your project. It will be responsible for a variety of common tasks, declare global variables, load functions, declare paths, and more. Every other file in the project will begin with source(&quot;0-config&quot;), and its role is to reduce redundancy and create an abstraction layer that allows you to make changes in one place (0-config.R) rather than 5 different files. To this end, paths that will be referenced in multiple scripts (e.g., a clean_data_path) can be declared in 0-config.R and simply referred to by its variable name in scripts. If you ever want to change things, rename them, or even switch from a downsample to the full data, all you would then to need to do is modify the path in one place and the change will automatically update throughout your project. See the example config file for more details. The paths defined in the 0-config.R file assume that users have opened the .Rproj file, which sets the directory to the top level of the project. 3.3.3 Order Files and Subdirectories This makes the jumble of alphabetized filenames much more coherent and places similar code and files next to one another. Although sometimes there is not a linear progression from 1 to 2 to 3, in general the structure helps reflect how data flows from start to finish and allows us to easily map a script to its output (i.e. primary-analysis/R/5-laz/1-laz-unadjusted-analysis.R =&gt; primary-analysis/output/5-laz/1-laz-unadjusted-analysis.RDS). That is, the code repository and the output are approximately mirrored. If you take nothing else away from this guide, this is the single most helpful suggestion to make your workflow more coherent. Often the particular order of files will be in flux until an analysis is close to completion. At that time it is important to review file order and naming and reproduce everything prior to drafting a manuscript. In the 6-figures subdirectory, each RMarkdown file (computational notebook) is linked to a specific figure in a hypothetical manuscript. This makes it easier to link specific notebooks in figure legends, and to see which file creates each figure. 3.3.4 Use Bash scripts to ensure reproducibility Bash scripts are useful components of a reproducible workflow. At many of the directory levels (i.e. in 5-laz), there is a bash script that runs each of the analysis scripts. This is exceptionally useful when data “upstream” changes – you simply run the bash script. See the UNIX Chapter for further details. 3.3.5 Alternative approach for code repos Another approach for organizing your code repository is to name all of your scripts according to the final figure or table that they generate for a particular article. In our experience, this only works for small projects, with a single set of coherent analyses. Here, you might have an alternative structure such as: .gitignore primary-analysis.Rproj 0-config.R 0-primary-analysis-run-all.sh 1-dm / 0-dm-run-all.sh 1-format-enrollment-data.R 2-format-adherence-data.R 3-format-LAZ-measurements.R Fig1-consort.Rmd Fig2-adherence.Rmd Fig3-1-laz-analysis.Rmd Fig3-2-laz-make-figure.Rmd There is still a need for a separate data management directory (e.g., dm) to ensure that workflow is upstream from the analysis (more below in chapter on UNIX), but then scripts are all together with clear labels. If a figure requires two stages to the analysis, then you can name them sequentially, such as Fig3-1-laz-analysis.Rmd, Fig3-2-laz-make-figure.Rmd. There is no way to divine how all of the analyses will neatly fit into files that correspond to separate figures. Instead, they will converge on these file names through the writing process, often through comsolidation or recombination. One example of a small repo is here: https://github.com/ben-arnold/enterics-seroepi "],
["codingpractices.html", "Chapter 4 Coding Practices 4.1 Organizing scripts 4.2 Documenting your code 4.3 Object naming 4.4 Function calls 4.5 The here package 4.6 Data input/output (I/O) 4.7 Tidyverse 4.8 Coding with R and Python", " Chapter 4 Coding Practices Contributors: Kunal Mishra, Jade Benjamin-Chung, Ben Arnold 4.1 Organizing scripts Just as your data “flows” through your project, data should flow naturally through a script. Very generally, you want to: describe the work completed in the script in a comment header source your configuration file (0-config.R) load all of your data do all your analysis/computation in order save your results Each section should be “chunked together” using comments, often with many chunks in a single section. See this file for a good example of how to cleanly organize a file in a way that follows this “flow” and functionally separate pieces of code that are doing different things. This is another example in a .Rmd format, where chunking is made even more obvious by the interleaving of markdown text and R code in the same notebook file. 4.2 Documenting your code 4.2.1 File headers Every file in a project should have a header that allows it to be interpreted on its own. It should include the name of the project and a short description for what this file (among the many in your project) does specifically. You may optionally wish to include the inputs and outputs of the script as well, though the next section makes this significantly less necessary. It can be very helpful to include your name and email address as well so others can identify who wrote the code. This is unnecessary if you are using version control (git/GitHub) becuase that information will be tracked by commits. #------------------------------------------------------------------------------- # @Organization - Example Organization # @Project - Example Project # @Author - Your name, and possibly email address (if appropriate) # @Description - This file is responsible for [...] #------------------------------------------------------------------------------- Consider using RStudio’s code folding feature to collapse and expand different sections of your code. Any comment line with at least four trailing dashes (-), equal signs (=), or pound signs (#) automatically creates a code section. Delimiters for chunks are a personal preference and all work equally well. For example: # Section 1 ------------------ works equally well as ############################## # Section 1 ################## ############################## 4.2.2 Comments in the body of your script Commenting your code is an important part of reproducibility and helps document your code for the future. When things change or break, you’ll be thankful for comments. When you revisit code you wrote two years earlier, you’ll be thankful for comments. There’s no need to comment excessively or unnecessarily, but a comment describing what a large or complex chunk of code does is always helpful. See this file for an example of how to comment your code and notice that comments are always in the form of: # This is a comment -- first letter is capitalized and spaced away from the pound sign 4.2.3 Function documentation Every function you write must include a header to document its purpose, inputs, and outputs. For any reproducible workflows, they are essential, because R is dynamically typed. This means, you can pass a string into an argument that is meant to be a data.table, or a list into an argument meant for a tibble. It is the responsibility of a function’s author to document what each argument is meant to do and its basic type. This is an example for documenting a function (inspired by JavaDocs, R’s Plumber API docs, and Roxygen2): #----------------------------------------------------------------------------- # Documentation: calc_fluseas_mean # Usage: calc_fluseas_mean(data, yname) # @description: Make a dataframe with rows for flu season and site # and the number of patients with an outcome, the total patients, # and the percent of patients with the outcome # Arguments/Options: # @param data: a data frame with variables flu_season, site, studyID, and yname # @param yname: a string for the outcome name # @param silent: a boolean specifying whether the function shouldn&#39;t output anything to the console (DEFAULT: TRUE) # @return: the dataframe as described above # @output: prints the data frame described above if silent is not True #----------------------------------------------------------------------------- calc_fluseas_mean = function(data, yname, silent = TRUE) { ### function code here } The header tells you what the function does, its various inputs, and how you might go about using the function to do what you want. Also notice that all optional arguments (i.e. ones with pre-specified defaults) follow arguments that require user input. Note: As someone trying to call a function, it is possible to access a function’s documentation (and internal code) by CMD-Left-Clicking the function’s name in RStudio Note: Depending on how important your function is, the complexity of your function code, and the complexity of different types of data in your project, you can also add “type-checking” to your function with the assertthat::assert_that() function. You can, for example, assert_that(is.data.frame(statistical_input)), which will ensure that collaborators or reviewers of your project attempting to use your function are using it in the way that it is intended by calling it with (at the minimum) the correct type of arguments. You can extend this to ensure that certain assumptions regarding the inputs are fulfilled as well (i.e. that time_column, location_column, value_column, and population_column all exist within the statistical_input tibble). 4.3 Object naming Generally we recommend using nouns for objects and verbs for functions. This is because functions are performing actions, while objects are not. Try to make your variable names both more expressive and more explicit. Being a bit more verbose is useful and easy in the age of autocompletion! For example, instead of naming a variable vaxcov_1718, try naming it vaccination_coverage_2017_18. Similarly, flu_res could be named absentee_flu_residuals, making your code more readable and explicit. For more help, check out Be Expressive: How to Give Your Variables Better Names We recommend you use Snake_Case. Base R allows . in variable names and functions (such as read.csv()), but this goes against best practices for variable naming in many other coding languages. For consistency’s sake, snake_case has been adopted across languages, and modern packages and functions typically use it (i.e. readr::read_csv()). As a very general rule of thumb, if a package you’re using doesn’t use snake_case, there may be an updated version or more modern package that does, bringing with it the variety of performance improvements and bug fixes inherent in more mature and modern software. Note: you may also see camelCase throughout the R code you come across. This is okay but not ideal – try to stay consistent across all your code with snake_case. Note: there is nothing inherently wrong with using . in variable names, just that it goes against style best practices that are cropping up in data science, so its worth getting rid of these bad habits now. 4.4 Function calls In a function call, use “named arguments” and separate arguments by to make your code more readable. Here’s an example of what not to do when calling the function a function calc_fluseas_mean (defined above): mean_Y = calc_fluseas_mean(flu_data, &quot;maari_yn&quot;, FALSE) And here it is again using the best practices we’ve outlined: mean_Y = calc_fluseas_mean( data = flu_data, yname = &quot;maari_yn&quot;, silent = FALSE ) 4.5 The here package The here package is one great R package that helps multiple collaborators deal with the mess that is working directories within an R project structure. Let’s say we have an R project at the path /home/oski/Some-R-Project. My collaborator might clone the repository and work with it at some other path, such as /home/bear/R-Code/Some-R-Project. Dealing with working directories and paths explicitly can be a very large pain, and as you might imagine, setting up a Config with paths requires those paths to flexibly work for all contributors to a project. This is where the here package comes in and this a great vignette describing it. For more motivation on why you should use the here and R projects (.Rproj), read this excellent blog post from Tidyverse. 4.6 Data input/output (I/O) 4.6.1 .RDS vs .RData Files One of the most common ways to load and save data in Base R is with the load() and save() functions to serialize multiple objects in a single .RData file. The biggest problems with this practice include an inability to control the names of things getting loaded in, the inherent confusion this creates in understanding older code, and the inability to load individual elements of a saved file. For this, we recommend using the RDS format to save R objects using saveRDS() and its complement readRDS(). Note: if you have many related R objects you would have otherwise saved all together using the save function, the functional equivalent with RDS would be to create a (named) list containing each of these objects, and saving it. Note: there is an important caveat for .rds files: they are not automatically backward compatible across different versions of R! So, while they are very useful in general, beware. See, for example, this thread on StackExchange. .csv files embed slightly less information (typically), but are more stable across different versions of R. 4.6.2 CSVs Once again, the readr package as part of the Tidvyerse is great, with a much faster read_csv() than Base R’s read.csv(). For massive CSVs (&gt; 5 GB), you’ll find data.table::fread() to be the fastest CSV reader in any data science language out there. For writing CSVs, readr::write_csv() and data.table::fwrite() outclass Base R’s write.csv() by a significant margin as well. 4.6.3 Publishing public data NEVER push a dataset into the public domain (e.g., GitHub, OSF) without first checking with Ben to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so. If you are releasing data into the public domain, then consider making available at minimum a .csv file and a codebook of the same name (note: you should have a codebook for internal data as well). We often also make available .rds files as well. For example, your mystudy/data/public directory could include three files for a single dataset, two with the actual data in .rds and .csv formats, and a third that describes their contents: analysis_data_public.csv analysis_data_public.rds analysis_data_public_codebook.txt In general, datasets are usually too big to save on GitHub, but occasionally they are small. Here is an example of where we actually pushed the data directly to GitHub: https://github.com/ben-arnold/enterics-seroepi/tree/master/data . If you inspect the codebooks you can see an example of what they could contain (overview, variable descriptions, references). If the data are bigger, then maintaining them under version control in your git repository can be unweildy. Instead, we recommend using another stable repository that has version control, such as the Open Science Framework (osf.io). For example, all of the data from the WASH Benefits trials (led by investigators at Berkeley, icddr,b, IPA-Kenya and others) are all stored through data components nested within in OSF projects: https://osf.io/tprw2/. 4.7 Tidyverse Throughout this document there have been references to the Tidyverse, but this section is to explicitly show you how to transform your Base R tendencies to Tidyverse (or Data.Table, Tidyverse’s performance-optimized competitor). For most of our work that does not utilize very large datasets, we recommend that you code in Tidyverse rather than Base R. Tidyverse is quickly becoming the gold standard in R data analysis and modern data science packages and code should use Tidyverse style and packages unless there’s a significant reason not to (i.e. big data pipelines that would benefit from Data.Table’s performance optimizations). The package author has published a great textbook on R for Data Science, which leans heavily on many Tidyverse packages and may be worth checking out. The following list is not exhaustive, but is a compact overview to begin to translate Base R into something better: Base R Better Style, Performance, and Utility _ _ read.csv() readr::read_csv() or data.table::fread() write.csv() readr::write_csv() or data.table::fwrite() readRDS readr::read_rds() saveRDS() readr::write_rds() _ _ data.frame() tibble::tibble() or data.table::data.table() rbind() dplyr::bind_rows() cbind() dplyr::bind_cols() df$some_column df %&gt;% dplyr::pull(some_column) df$some_column = ... df %&gt;% dplyr::mutate(some_column = ...) df[get_rows_condition,] df %&gt;% dplyr::filter(get_rows_condition) df[,c(col1, col2)] df %&gt;% dplyr::select(col1, col2) merge(df1, df2, by = ..., all.x = ..., all.y = ...) df1 %&gt;% dplyr::left_join(df2, by = ...) or dplyr::full_join or dplyr::inner_join or dplyr::right_join _ _ str() dplyr::glimpse() grep(pattern, x) stringr::str_which(string, pattern) gsub(pattern, replacement, x) stringr::str_replace(string, pattern, replacement) ifelse(test_expression, yes, no) if_else(condition, true, false) Nested: ifelse(test_expression1, yes1, ifelse(test_expression2, yes2, ifelse(test_expression3, yes3, no))) case_when(test_expression1 ~ yes1, test_expression2 ~ yes2, test_expression3 ~ yes3, TRUE ~ no) proc.time() tictoc::tic() and tictoc::toc() stopifnot() assertthat::assert_that() or assertthat::see_if() or assertthat::validate_that() For a more extensive set of syntactical translations to Tidyverse, you can check out this document. Working with Tidyverse within functions can be somewhat of a pain due to non-standard evaluation (NSE) semantics. If you’re an avid function writer, we’d recommend checking out the following resources: Tidy Eval in 5 Minutes (video) Tidy Evaluation (e-book) Data Frame Columns as Arguments to Dplyr Functions (blog) Standard Evaluation for *_join (stackoverflow) Programming with dplyr (package vignette) 4.8 Coding with R and Python If you’re using both R and Python, you may wish to check out the Feather package for exchanging data between the two languages extremely quickly. "],
["codingstyle.html", "Chapter 5 Coding Style", " Chapter 5 Coding Style TBD "],
["datamanagement.html", "Chapter 6 Data Management", " Chapter 6 Data Management TBD "],
["github.html", "Chapter 7 GitHub and Version Control", " Chapter 7 GitHub and Version Control TBD "],
["bigdata.html", "Chapter 8 Working with Big Data", " Chapter 8 Working with Big Data TBD "],
["unix.html", "Chapter 9 UNIX Commands", " Chapter 9 UNIX Commands TBD "],
["commcoord.html", "Chapter 10 Communication and Coordination 10.1 Slack 10.2 Email 10.3 Trello 10.4 Google Drive 10.5 Calendar / Meetings", " Chapter 10 Communication and Coordination Contributors: Jade Benjamin-Chung, Ben Arnold These communications guidelines are evolving as we increasingly adopt Slack, but here some general principles if you work closely with Ben. 10.1 Slack If you work with Ben but are not a member of Ben’s Slack workspace then ask him to invite you! Use Slack for scheduling, coding related questions, quick check ins, etc. If your Slack message exceeds 200 words, it might be time to use email. Use channels instead of direct messages unless you need to discuss something private. Include tags on your message (e.g., @Ben) when you want to ensure that a person sees the message. Ben doesn’t regularly read messages where he isn’t tagged. Please make an effort to respond to messages that message you (e.g., @Ben) as quickly as possible and always within 24 hours, unless of course you are on vacation! If you are unusually busy (e.g., taking MCAT/GRE, taking many exams) or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual and also set your status in Slack (e.g., it could say “On vacation”) so we know not to expect to see you online. Please thread messages in Slack as much as possible. 10.2 Email Use email for longer messages (&gt;200 words) or messages that merit preservation. Generally, strive to respond within 24 hours. If you are unusually busy or on vacation please alert the team in advance so we can expect you not to respond at all / as quickly as usual. 10.3 Trello Ben manages projects and teams using a kanban board approach in Trello. You and/or Ben will add new cards within our team’s Trello boards and assign them to team members. Each card represents a discrete chunk of work. Cards higher in a list are higher priority. Strive to complete the tasks in your card by the card’s due date. Talk to Ben about deadlines – we can always manage the calendar! Use checklists to break down a task into smaller chunks. Usually, you can do this yourself (but ask Ben if you ever want input). Move cards to the “DONE” list on a board when they are done. 10.4 Google Drive We mostly use Google Drive to create shared documents with longer descriptions of tasks. These documents are linked to Trello cards. Ben often shares these docs with a whole project team since tasks are overlapping, and even if a task is assigned to one person, others may have valuable insights. Please invite both of Ben’s email addresses to any documents you create (bfarnold@gmail.com, ben.arnold@ucsf.edu). 10.5 Calendar / Meetings Ben will schedule most meetings through the calendar. Our meetings start on the hour. If you are going to be late, please send a message in our Slack channel. If you are regularly not able to come on the hour, notify the team and we might choose the modify the agenda order or the start time. Add hoc meetings are welcome. If Ben’s office door is open, come in! "],
["code-of-conduct.html", "Chapter 11 Code of conduct 11.1 Group culture 11.2 Protecting human subjects 11.3 Authorship 11.4 Work hours", " Chapter 11 Code of conduct Contributors: Jade Benjamin-Chung, Ben Arnold 11.1 Group culture We strive to work in an environment that is collaborative, supportive, open, and free from discrimination and harassment, per University policies. We encourage students / staff of all experience levels to respectfully share their honest opinions and ideas on any topic. Our group has thrived upon such respectful honest input from team members over the years, and this document is a product of years of student and staff input (and even debate) that has gradually improved our productivity and overall quality of our work. If Ben is your PI, be forewarned that he tends to batch his email communication (~30 mins in the morning and afternoon, 15 mins mid-day), and doesn’t tend to answer Slack or email during evenings or weekends. If you need to reach him urgently then give him a call or text on his mobile. 11.2 Protecting human subjects All lab members must complete CITI Human Subjects Training and share their certificate with Ben. We will will add team members to relevant Institutional Review Board protocols to ensure they have permission to work with identifiable datasets. One of the most relevant aspects of protecting human subjects in our work in the Data Coordinating Center is maintaining confidentiality and data privacy. For students supporting our data science efforts, in practice this means: If you are using a virtual computer (e.g., Google Cloud, AWS, Optum), never save the data in that system to your personal computer or any other computer without prior permission. Do not share data with anyone without first obtaining permission, including to other members of the Proctor Foundation, who might not be on the same IRB protocol as you (check with Ben or the relevant PI first). NEVER push a dataset into the public domain (e.g., GitHub, OSF) without first checking with Ben to ensure that it is appropriately de-identified and we have approval from the sponsor and/or human subjects review board to do so. Remember, data that looks like it does not contain identifiers to you might still be classified as data that requires special protection by our IRB or under HIPAA, so always proceed with caution and ask for help if you have any concerns about how to maintain study participant confidentiality. For example, the combination of age, sex, and geographic location of the individual’s town or neighborhood is typically considered identifiable. 11.3 Authorship We adhere to the ICMJE Definition of authorship and are happy for team members who meet the definition of authorship to be included as co-authors on scientific manuscripts. 11.4 Work hours Please follow the Proctor Foundation’s employee guidelines for work hours, and discuss the specifics with your PI. If Ben is your PI, then work with him on your schedule to ensure we have overlap in the office and that you are around at key times for group meetings, etc. "],
["resources.html", "Chapter 12 Additional Resources", " Chapter 12 Additional Resources TBD "],
["references.html", "References", " References "]
]
